{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "# Project Title\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Rama Iyer\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This project is my capstone submission for the Udacity Data Engineering Nanodegree. It showcases what I've learned through the program by taking large sets of data and organizing it in such a way that allows others to gain valuable insight from it.\n",
    "\n",
    "This project creates an analytical datawarehouse for consumption by US Government, which can be used to understand immigration patterns, and help in making policy decisions around immigration so that industries such as tourism, business and higher education sector are able to thrive successfully. \n",
    "\n",
    "The analytical warehouse is hosted on Amazon Redshift and the pipeline implementation was done using Apache Airflow.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 0: Imports\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Step 0 - Imports\n",
    "# Do all imports and installs here\n",
    "import configparser\n",
    "import datetime as dt\n",
    "import os\n",
    "import dataMappings\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import dayofweek, monotonically_increasing_id, from_unixtime\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType, TimestampType,DateType, FloatType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import types \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/workspace/dl.cfg']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('/home/workspace/dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS','AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Return a SparkSession object.\"\"\"\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\",\"false\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    spark.conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "#### Scope\n",
    "In this project an analytical database will be made available for the USA government, so they can quickly gather insights from the data.\n",
    "\n",
    "At a high level,  \n",
    "The data is cleaned and loaded into S3 as parquet files.  \n",
    "This is then loaded into Redshift using pipeline created using Airflow.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Data Sets Used:  \n",
    "The following data was used to build the datawarehouse:  \n",
    "\n",
    "I94 Immigration Data:  \n",
    "The immigration data comes from the [US National Tourism and Trade Office](https://www.trade.gov/national-travel-and-tourism-office). It includes information about people entering the United States, such as immigration year and month, arrival and departure dates, age and year of birth of immigrant, arrival city, port, current residence state, travel mode (air, sea), visa type etc. Specificially, the data from April 2016 is used to showcase this project, which is over 1 million records. The data is in parquet format.    \n",
    "\n",
    "U.S. City Demographic Data:  \n",
    "The demographic data comes from [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/). It includes demographic information about US cities, such as the median age, total population, and specific populations (male vs female, foreign-born, different races, etc.). The data is in csv format.  \n",
    "\n",
    "Country Data  \n",
    "This data was provided in I94_SAS_Labels_Descriptions.SAS in the provided project and contains a mapping of country names and their I94 codes that are found in the immigration data. I used Spark to create a parquet file with this information.  \n",
    "\n",
    "Visa Codes  \n",
    "This data was provided in I94_SAS_Labels_Descriptions.SAS in the provided project. This maps to the \n",
    "I94VISA column in the immigration table. I used Spark to create a parquet file with this information.  \n",
    "\n",
    "State Code  \n",
    "This data was provided in I94_SAS_Labels_Descriptions.SAS in the provided project. This maps to the \n",
    "I94ADDR column in the immigration table. I used Spark to create a parquet file with this information.  \n",
    "\n",
    "Travel Mode  \n",
    "This data was provided in I94_SAS_Labels_Descriptions.SAS in the provided project. This maps to the \n",
    "I94MODE column in the immigration table. I used Spark to create a parquet file with this information.  \n",
    "\n",
    "Port Code    \n",
    "This data was provided in I94_SAS_Labels_Descriptions.SAS in the provided project. This maps to the \n",
    "I94PORT column in the immigration table. I used Spark to create a parquet file with this information.  \n",
    "\n",
    "Airport Code Table: This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read in the data here\n",
    "Read the cities demographic file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(delimiter=\";\").csv(\"us-cities-demographics.csv\",header=True)\n",
    "#print schems to get idea of columns in the data frame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show data \n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|TotalRecords|DistinctCity|\n",
      "+------------+------------+\n",
      "|        2891|         567|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create Temporary view on dataframe and explore\n",
    "df.createOrReplaceTempView(\"CityTable\")\n",
    "dSQL = spark.sql(\"Select count(*) AS TotalRecords, count(distinct City) AS DistinctCity from CityTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------+\n",
      "|                City|   State|DistinctRace|\n",
      "+--------------------+--------+------------+\n",
      "|          Cincinnati|    Ohio|           5|\n",
      "|           Lynchburg|Virginia|           5|\n",
      "|         Kansas City|  Kansas|           5|\n",
      "|Louisville/Jeffer...|Kentucky|           5|\n",
      "|              Dayton|    Ohio|           5|\n",
      "+--------------------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dSQL = spark.sql('''\n",
    "Select City, State, count(distinct Race) AS DistinctRace \n",
    "from CityTable Group by City, State\n",
    "''').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT Race)|\n",
      "+--------------------+\n",
      "|                   5|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dSQL = spark.sql(\"select count(distinct Race) from CityTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Exploration of the above demographics data tells us that there are a total of 2891 records with 567 distinct cities and only 5 distinct Race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean the data - Demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "City Demographics - Change the data type of columns to the appropriate type. Also, as there is only 5 distinct Race in the data set, pivot the data by Race for each city and state combination and write this new data set to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>State</th>\n",
       "      <th>MedianAge</th>\n",
       "      <th>MalePopulation</th>\n",
       "      <th>FemalePopulation</th>\n",
       "      <th>TotalPopulation</th>\n",
       "      <th>NumberOfVeterans</th>\n",
       "      <th>ForeignBorn</th>\n",
       "      <th>AverageHouseholdSize</th>\n",
       "      <th>AmericanIndianandAlaskanNativePopulation</th>\n",
       "      <th>AsianPopulation</th>\n",
       "      <th>BlackorAfricanAmericanPopulation</th>\n",
       "      <th>HispanicorLatinoPopulation</th>\n",
       "      <th>WhitePopulation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trenton</td>\n",
       "      <td>NJ</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>33.299999</td>\n",
       "      <td>42581</td>\n",
       "      <td>41650</td>\n",
       "      <td>84231</td>\n",
       "      <td>2604</td>\n",
       "      <td>20428</td>\n",
       "      <td>3.04</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1437</td>\n",
       "      <td>43471</td>\n",
       "      <td>30613</td>\n",
       "      <td>37683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Saint Cloud</td>\n",
       "      <td>MN</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>34311</td>\n",
       "      <td>33942</td>\n",
       "      <td>68253</td>\n",
       "      <td>5012</td>\n",
       "      <td>5757</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>2494</td>\n",
       "      <td>6401</td>\n",
       "      <td>2639</td>\n",
       "      <td>59794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cambridge</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>55421</td>\n",
       "      <td>54981</td>\n",
       "      <td>110402</td>\n",
       "      <td>2495</td>\n",
       "      <td>27757</td>\n",
       "      <td>2.06</td>\n",
       "      <td>906.0</td>\n",
       "      <td>18441</td>\n",
       "      <td>11880</td>\n",
       "      <td>9433</td>\n",
       "      <td>80551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spring Hill</td>\n",
       "      <td>FL</td>\n",
       "      <td>Florida</td>\n",
       "      <td>45.099998</td>\n",
       "      <td>48295</td>\n",
       "      <td>54902</td>\n",
       "      <td>103197</td>\n",
       "      <td>9737</td>\n",
       "      <td>7893</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1254.0</td>\n",
       "      <td>2409</td>\n",
       "      <td>7689</td>\n",
       "      <td>15838</td>\n",
       "      <td>91742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Missouri City</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>34932</td>\n",
       "      <td>36846</td>\n",
       "      <td>71778</td>\n",
       "      <td>4274</td>\n",
       "      <td>18556</td>\n",
       "      <td>3.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17854</td>\n",
       "      <td>28070</td>\n",
       "      <td>10007</td>\n",
       "      <td>20590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City StateCode          State  MedianAge  MalePopulation  \\\n",
       "0        Trenton        NJ     New Jersey  33.299999           42581   \n",
       "1    Saint Cloud        MN      Minnesota  30.700001           34311   \n",
       "2      Cambridge        MA  Massachusetts  31.500000           55421   \n",
       "3    Spring Hill        FL        Florida  45.099998           48295   \n",
       "4  Missouri City        TX          Texas  37.200001           34932   \n",
       "\n",
       "   FemalePopulation  TotalPopulation  NumberOfVeterans  ForeignBorn  \\\n",
       "0             41650            84231              2604        20428   \n",
       "1             33942            68253              5012         5757   \n",
       "2             54981           110402              2495        27757   \n",
       "3             54902           103197              9737         7893   \n",
       "4             36846            71778              4274        18556   \n",
       "\n",
       "   AverageHouseholdSize  AmericanIndianandAlaskanNativePopulation  \\\n",
       "0                  3.04                                      98.0   \n",
       "1                  2.41                                    1169.0   \n",
       "2                  2.06                                     906.0   \n",
       "3                  2.58                                    1254.0   \n",
       "4                  3.03                                       NaN   \n",
       "\n",
       "   AsianPopulation  BlackorAfricanAmericanPopulation  \\\n",
       "0             1437                             43471   \n",
       "1             2494                              6401   \n",
       "2            18441                             11880   \n",
       "3             2409                              7689   \n",
       "4            17854                             28070   \n",
       "\n",
       "   HispanicorLatinoPopulation  WhitePopulation  \n",
       "0                       30613            37683  \n",
       "1                        2639            59794  \n",
       "2                        9433            80551  \n",
       "3                       15838            91742  \n",
       "4                       10007            20590  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename columms and Change datatypes\n",
    "dSQL = spark.sql(\"Select City, \\\n",
    "                         State, \\\n",
    "                         float(`Median Age`) MedianAge, \\\n",
    "                         int(`Male Population`) MalePopulation, \\\n",
    "                         int(`Female Population`) FemalePopulation , \\\n",
    "                         int(`Total Population`) TotalPopulation, \\\n",
    "                         int(`Number of Veterans`) NumberOfVeterans, \\\n",
    "                         int(`Foreign-Born`) ForeignBorn, \\\n",
    "                         float(`Average Household Size`) AverageHouseholdSize, \\\n",
    "                         `State Code` StateCode, \\\n",
    "                         Race, \\\n",
    "                         int(Count) Count \\\n",
    "                         from CityTable\")\n",
    "\n",
    "# Pivot the dataframe by Race to get count in a single row\n",
    "d_pivot_df = dSQL.groupBy(\"City\",\"StateCode\").pivot(\"Race\").sum(\"Count\")\n",
    "\n",
    "#join the two datasets to create a single combined final dataset and drop duplicates\n",
    "f_df = dSQL.join(d_pivot_df,[\"City\",\"StateCode\"]).drop(\"Race\",\"Count\").dropDuplicates()\n",
    "\n",
    "#rename columms \n",
    "f_df = f_df.withColumnRenamed(\"American Indian and Alaska Native\",\"AmericanIndianandAlaskanNativePopulation\") \\\n",
    ".withColumnRenamed(\"Asian\",\"AsianPopulation\") \\\n",
    ".withColumnRenamed(\"Black or African-American\",\"BlackorAfricanAmericanPopulation\") \\\n",
    ".withColumnRenamed(\"Hispanic or Latino\",\"HispanicorLatinoPopulation\") \\\n",
    ".withColumnRenamed(\"White\", \"WhitePopulation\")\n",
    "f_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Read the immigration data file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read immigration data\n",
    "df = spark.read.parquet(\"sas_data\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------+\n",
      "|TotalRecords|DistinctImmigrationCount|\n",
      "+------------+------------------------+\n",
      "|     3096313|                 3096313|\n",
      "+------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create temporary view to explore \n",
    "df.createOrReplaceTempView(\"ImmigTable\")\n",
    "dSQL = spark.sql('''\n",
    "SELECT count(*) AS TotalRecords, count(distinct cicid ) AS DistinctImmigrationCount \n",
    "FROM ImmigTable\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|NoArrivalDate|\n",
      "+-------------+\n",
      "|            0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dSQL = spark.sql('''\n",
    "SELECT count(*) AS NoArrivalDate FROM ImmigTable WHERE arrdate is NULL\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|IncorrectDeparture|\n",
      "+------------------+\n",
      "|            142457|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dSQL = spark.sql('''\n",
    "SELECT count(*) AS IncorrectDeparture FROM ImmigTable WHERE depdate IS NULL\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----------+-----------+-----------+\n",
      "|min_i94yr|max_i94yr|min_i94mon|max_i94mon|min_arrdate|min_depdate|\n",
      "+---------+---------+----------+----------+-----------+-----------+\n",
      "|   2016.0|   2016.0|       4.0|       4.0|    20545.0|    15176.0|\n",
      "+---------+---------+----------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dSQL = spark.sql('''\n",
    "SELECT min(i94yr) min_i94yr, max(i94yr) as max_i94yr, min(i94mon) as min_i94mon, max(i94mon)  as max_i94mon,\n",
    "min(arrdate) as min_arrdate, min(depdate) as min_depdate\n",
    "FROM ImmigTable \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|IncorrectDeparture|\n",
      "+------------------+\n",
      "|               375|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dSQL = spark.sql('''\n",
    "SELECT count(*) AS IncorrectDeparture FROM ImmigTable WHERE depdate < arrdate\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The immigration data has a total of 3096313 records and the same number of distinct immigration id. Hence each row of data is unique. There are no records without an arrival date. Dates are represented as numbers. There are 375 records where the departure date is earlier to the arrival date. These need to be cleaned. There are 142457 records that have no departure dates, these are valid though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean the data - Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The dates in the immigration data are in double format, these need to be converted to date data type. Also convert the data types to suitable types for the other columns as well. The below columns are not used for analysis and hence can be dropped from the final data frame.\n",
    "DTADFILE,VISAPOST,OCCUP,ENTDEPA,ENTDEPD,ENTDEPU,DTADDTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# to convert to date \n",
    "get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(float(x))).isoformat() if x else None)\n",
    "df = df.withColumn(\"arrival_date\", get_date(df.arrdate))\n",
    "df = df.withColumn(\"departure_date\", get_date(df.depdate))\n",
    "\n",
    "df = df.withColumn(\"arrival_date\", to_date(df.arrival_date, 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"departure_date\", to_date(df.departure_date, 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep_year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2069.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>142457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>2953826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2084.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dep_year    count\n",
       "0    2069.0        1\n",
       "1       NaN   142457\n",
       "2    2020.0        1\n",
       "3    2016.0  2953826\n",
       "4    2084.0        1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"ImmigData\")\n",
    "dSQL = spark.sql('''\n",
    "                    select \n",
    "                        int(cicid) immigration_id,\n",
    "                        int(i94yr) immigration_year,\n",
    "                        int(i94mon) immigration_month,\n",
    "                        int(i94cit) citizenship_country_code,\n",
    "                        int(i94res) residency_country_code,\n",
    "                        i94port port_code,\n",
    "                        arrival_date,\n",
    "                        int(i94mode) travel_mode,\n",
    "                        i94addr current_state_code,\n",
    "                        departure_date,\n",
    "                        int(i94bir) age,\n",
    "                        int(i94visa) visa_code,\n",
    "                        matflag match_flag,\n",
    "                        int(biryear) birth_year,\n",
    "                        gender,\n",
    "                        airline airline_code,\n",
    "                        bigint(admnum) admission_num,\n",
    "                        visatype visa_type,\n",
    "                        int(i94yr) immigration_year_part,\n",
    "                        int(i94mon) immigration_month_part\n",
    "                    from ImmigData     \n",
    "                    WHERE departure_date is null or year(departure_date) >= 2016\n",
    "               ''')\n",
    "dSQL.select(year(\"departure_date\").alias(\"dep_year\")).groupBy(\"dep_year\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>immigration_id</th>\n",
       "      <th>immigration_year</th>\n",
       "      <th>immigration_month</th>\n",
       "      <th>citizenship_country_code</th>\n",
       "      <th>residency_country_code</th>\n",
       "      <th>port_code</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>travel_mode</th>\n",
       "      <th>current_state_code</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>age</th>\n",
       "      <th>visa_code</th>\n",
       "      <th>match_flag</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>airline_code</th>\n",
       "      <th>admission_num</th>\n",
       "      <th>visa_type</th>\n",
       "      <th>immigration_year_part</th>\n",
       "      <th>immigration_month_part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2016-05-08</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1976</td>\n",
       "      <td>F</td>\n",
       "      <td>QF</td>\n",
       "      <td>94953870030</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>NV</td>\n",
       "      <td>2016-05-17</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1984</td>\n",
       "      <td>F</td>\n",
       "      <td>VA</td>\n",
       "      <td>94955622830</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>WA</td>\n",
       "      <td>2016-05-08</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1987</td>\n",
       "      <td>M</td>\n",
       "      <td>DL</td>\n",
       "      <td>94956406530</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>WA</td>\n",
       "      <td>2016-05-14</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1987</td>\n",
       "      <td>F</td>\n",
       "      <td>DL</td>\n",
       "      <td>94956451430</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>245</td>\n",
       "      <td>438</td>\n",
       "      <td>LOS</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>WA</td>\n",
       "      <td>2016-05-14</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1988</td>\n",
       "      <td>M</td>\n",
       "      <td>DL</td>\n",
       "      <td>94956388130</td>\n",
       "      <td>B1</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   immigration_id  immigration_year  immigration_month  \\\n",
       "0         5748517              2016                  4   \n",
       "1         5748518              2016                  4   \n",
       "2         5748519              2016                  4   \n",
       "3         5748520              2016                  4   \n",
       "4         5748521              2016                  4   \n",
       "\n",
       "   citizenship_country_code  residency_country_code port_code arrival_date  \\\n",
       "0                       245                     438       LOS   2016-04-30   \n",
       "1                       245                     438       LOS   2016-04-30   \n",
       "2                       245                     438       LOS   2016-04-30   \n",
       "3                       245                     438       LOS   2016-04-30   \n",
       "4                       245                     438       LOS   2016-04-30   \n",
       "\n",
       "   travel_mode current_state_code departure_date  age  visa_code match_flag  \\\n",
       "0            1                 CA     2016-05-08   40          1          M   \n",
       "1            1                 NV     2016-05-17   32          1          M   \n",
       "2            1                 WA     2016-05-08   29          1          M   \n",
       "3            1                 WA     2016-05-14   29          1          M   \n",
       "4            1                 WA     2016-05-14   28          1          M   \n",
       "\n",
       "   birth_year gender airline_code  admission_num visa_type  \\\n",
       "0        1976      F           QF    94953870030        B1   \n",
       "1        1984      F           VA    94955622830        B1   \n",
       "2        1987      M           DL    94956406530        B1   \n",
       "3        1987      F           DL    94956451430        B1   \n",
       "4        1988      M           DL    94956388130        B1   \n",
       "\n",
       "   immigration_year_part  immigration_month_part  \n",
       "0                   2016                       4  \n",
       "1                   2016                       4  \n",
       "2                   2016                       4  \n",
       "3                   2016                       4  \n",
       "4                   2016                       4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dSQL.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Map Visa Codes, Port Codes, Country Codes and Travel mode from I94_SAS_Labels_Descriptions.SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visa_code</th>\n",
       "      <th>visa_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visa_code visa_type\n",
       "0          1  Business\n",
       "1          2  Pleasure\n",
       "2          3   Student"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visa codes dictionary\n",
    "visa_codes = {\n",
    "   1 : 'Business',\n",
    "   2 : 'Pleasure',\n",
    "   3 : 'Student'\n",
    "}\n",
    "\n",
    "df_visa_codes = spark.createDataFrame(list(map(list, visa_codes.items())),\n",
    "                                        [\"visa_code\",\"visa_type\"])\n",
    "df_visa_codes.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Country Code Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_code                                            country\n",
       "0           582  MEXICO Air Sea, and Not Reported (I-94, no lan...\n",
       "1           236                                        AFGHANISTAN\n",
       "2           101                                            ALBANIA\n",
       "3           316                                            ALGERIA\n",
       "4           102                                            ANDORRA"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit_and_res_codes = {  \n",
    "   582 :  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)',\n",
    "   236 :  'AFGHANISTAN',\n",
    "   101 :  'ALBANIA',\n",
    "   316 :  'ALGERIA',\n",
    "   102 :  'ANDORRA',\n",
    "   324 :  'ANGOLA',\n",
    "   529 :  'ANGUILLA',\n",
    "   518 :  'ANTIGUA-BARBUDA',\n",
    "   687 :  'ARGENTINA ',\n",
    "   151 :  'ARMENIA',\n",
    "   532 :  'ARUBA',\n",
    "   438 :  'AUSTRALIA',\n",
    "   103 :  'AUSTRIA',\n",
    "   152 :  'AZERBAIJAN',\n",
    "   512 :  'BAHAMAS',\n",
    "   298 :  'BAHRAIN',\n",
    "   274 :  'BANGLADESH',\n",
    "   513 :  'BARBADOS',\n",
    "   104 :  'BELGIUM',\n",
    "   581 :  'BELIZE',\n",
    "   386 :  'BENIN',\n",
    "   509 :  'BERMUDA',\n",
    "   153 :  'BELARUS',\n",
    "   242 :  'BHUTAN',\n",
    "   688 :  'BOLIVIA',\n",
    "   717 :  'BONAIRE, ST EUSTATIUS, SABA' ,\n",
    "   164 :  'BOSNIA-HERZEGOVINA',\n",
    "   336 :  'BOTSWANA',\n",
    "   689 :  'BRAZIL',\n",
    "   525 :  'BRITISH VIRGIN ISLANDS',\n",
    "   217 :  'BRUNEI',\n",
    "   105 :  'BULGARIA',\n",
    "   393 :  'BURKINA FASO',\n",
    "   243 :  'BURMA',\n",
    "   375 :  'BURUNDI',\n",
    "   310 :  'CAMEROON',\n",
    "   326 :  'CAPE VERDE',\n",
    "   526 :  'CAYMAN ISLANDS',\n",
    "   383 :  'CENTRAL AFRICAN REPUBLIC',\n",
    "   384 :  'CHAD',\n",
    "   690 :  'CHILE',\n",
    "   245 :  'CHINA, PRC',\n",
    "   721 :  'CURACAO' ,\n",
    "   270 :  'CHRISTMAS ISLAND',\n",
    "   271 :  'COCOS ISLANDS',\n",
    "   691 :  'COLOMBIA',\n",
    "   317 :  'COMOROS',\n",
    "   385 :  'CONGO',\n",
    "   467 :  'COOK ISLANDS',\n",
    "   575 :  'COSTA RICA',\n",
    "   165 :  'CROATIA',\n",
    "   584 :  'CUBA',\n",
    "   218 :  'CYPRUS',\n",
    "   140 :  'CZECH REPUBLIC',\n",
    "   723 :  'FAROE ISLANDS (PART OF DENMARK)'  ,\n",
    "   108 :  'DENMARK',\n",
    "   322 :  'DJIBOUTI',\n",
    "   519 :  'DOMINICA',\n",
    "   585 :  'DOMINICAN REPUBLIC',\n",
    "   240 :  'EAST TIMOR',\n",
    "   692 :  'ECUADOR',\n",
    "   368 :  'EGYPT',\n",
    "   576 :  'EL SALVADOR',\n",
    "   399 :  'EQUATORIAL GUINEA',\n",
    "   372 :  'ERITREA',\n",
    "   109 :  'ESTONIA',\n",
    "   369 :  'ETHIOPIA',\n",
    "   604 :  'FALKLAND ISLANDS',\n",
    "   413 :  'FIJI',\n",
    "   110 :  'FINLAND',\n",
    "   111 :  'FRANCE',\n",
    "   601 :  'FRENCH GUIANA',\n",
    "   411 :  'FRENCH POLYNESIA',\n",
    "   387 :  'GABON',\n",
    "   338 :  'GAMBIA',\n",
    "   758 :  'GAZA STRIP' ,\n",
    "   154 :  'GEORGIA',\n",
    "   112 :  'GERMANY',\n",
    "   339 :  'GHANA',\n",
    "   143 :  'GIBRALTAR',\n",
    "   113 :  'GREECE',\n",
    "   520 :  'GRENADA',\n",
    "   507 :  'GUADELOUPE',\n",
    "   577 :  'GUATEMALA',\n",
    "   382 :  'GUINEA',\n",
    "   327 :  'GUINEA-BISSAU',\n",
    "   603 :  'GUYANA',\n",
    "   586 :  'HAITI',\n",
    "   726 :  'HEARD AND MCDONALD IS.',\n",
    "   149 :  'HOLY SEE/VATICAN',\n",
    "   528 :  'HONDURAS',\n",
    "   206 :  'HONG KONG',\n",
    "   114 :  'HUNGARY',\n",
    "   115 :  'ICELAND',\n",
    "   213 :  'INDIA',\n",
    "   759 :  'INDIAN OCEAN AREAS (FRENCH)' ,\n",
    "   729 :  'INDIAN OCEAN TERRITORY' ,\n",
    "   204 :  'INDONESIA',\n",
    "   249 :  'IRAN',\n",
    "   250 :  'IRAQ',\n",
    "   116 :  'IRELAND',\n",
    "   251 :  'ISRAEL',\n",
    "   117 :  'ITALY',\n",
    "   388 :  'IVORY COAST',\n",
    "   514 :  'JAMAICA',\n",
    "   209 :  'JAPAN',\n",
    "   253 :  'JORDAN',\n",
    "   201 :  'KAMPUCHEA',\n",
    "   155 :  'KAZAKHSTAN',\n",
    "   340 :  'KENYA',\n",
    "   414 :  'KIRIBATI',\n",
    "   732 :  'KOSOVO' ,\n",
    "   272 :  'KUWAIT',\n",
    "   156 :  'KYRGYZSTAN',\n",
    "   203 :  'LAOS',\n",
    "   118 :  'LATVIA',\n",
    "   255 :  'LEBANON',\n",
    "   335 :  'LESOTHO',\n",
    "   370 :  'LIBERIA',\n",
    "   381 :  'LIBYA',\n",
    "   119 :  'LIECHTENSTEIN',\n",
    "   120 :  'LITHUANIA',\n",
    "   121 :  'LUXEMBOURG',\n",
    "   214 :  'MACAU',\n",
    "   167 :  'MACEDONIA',\n",
    "   320 :  'MADAGASCAR',\n",
    "   345 :  'MALAWI',\n",
    "   273 :  'MALAYSIA',\n",
    "   220 :  'MALDIVES',\n",
    "   392 :  'MALI',\n",
    "   145 :  'MALTA',\n",
    "   472 :  'MARSHALL ISLANDS',\n",
    "   511 :  'MARTINIQUE',\n",
    "   389 :  'MAURITANIA',\n",
    "   342 :  'MAURITIUS',\n",
    "   760 :  'MAYOTTE (AFRICA - FRENCH)' ,\n",
    "   473 :  'MICRONESIA, FED. STATES OF',\n",
    "   157 :  'MOLDOVA',\n",
    "   122 :  'MONACO',\n",
    "   299 :  'MONGOLIA',\n",
    "   735 :  'MONTENEGRO' ,\n",
    "   521 :  'MONTSERRAT',\n",
    "   332 :  'MOROCCO',\n",
    "   329 :  'MOZAMBIQUE',\n",
    "   371 :  'NAMIBIA',\n",
    "   440 :  'NAURU',\n",
    "   257 :  'NEPAL',\n",
    "   123 :  'NETHERLANDS',\n",
    "   508 :  'NETHERLANDS ANTILLES',\n",
    "   409 :  'NEW CALEDONIA',\n",
    "   464 :  'NEW ZEALAND',\n",
    "   579 :  'NICARAGUA',\n",
    "   390 :  'NIGER',\n",
    "   343 :  'NIGERIA',\n",
    "   470 :  'NIUE',\n",
    "   275 :  'NORTH KOREA',\n",
    "   124 :  'NORWAY',\n",
    "   256 :  'OMAN',\n",
    "   258 :  'PAKISTAN',\n",
    "   474 :  'PALAU',\n",
    "   743 :  'PALESTINE' ,\n",
    "   504 :  'PANAMA',\n",
    "   441 :  'PAPUA NEW GUINEA',\n",
    "   693 :  'PARAGUAY',\n",
    "   694 :  'PERU',\n",
    "   260 :  'PHILIPPINES',\n",
    "   416 :  'PITCAIRN ISLANDS',\n",
    "   107 :  'POLAND',\n",
    "   126 :  'PORTUGAL',\n",
    "   297 :  'QATAR',\n",
    "   748 :  'REPUBLIC OF SOUTH SUDAN',\n",
    "   321 :  'REUNION',\n",
    "   127 :  'ROMANIA',\n",
    "   158 :  'RUSSIA',\n",
    "   376 :  'RWANDA',\n",
    "   128 :  'SAN MARINO',\n",
    "   330 :  'SAO TOME AND PRINCIPE',\n",
    "   261 :  'SAUDI ARABIA',\n",
    "   391 :  'SENEGAL',\n",
    "   142 :  'SERBIA AND MONTENEGRO',\n",
    "   745 :  'SERBIA' ,\n",
    "   347 :  'SEYCHELLES',\n",
    "   348 :  'SIERRA LEONE',\n",
    "   207 :  'SINGAPORE',\n",
    "   141 :  'SLOVAKIA',\n",
    "   166 :  'SLOVENIA',\n",
    "   412 :  'SOLOMON ISLANDS',\n",
    "   397 :  'SOMALIA',\n",
    "   373 :  'SOUTH AFRICA',\n",
    "   276 :  'SOUTH KOREA',\n",
    "   129 :  'SPAIN',\n",
    "   244 :  'SRI LANKA',\n",
    "   346 :  'ST. HELENA',\n",
    "   522 :  'ST. KITTS-NEVIS',\n",
    "   523 :  'ST. LUCIA',\n",
    "   502 :  'ST. PIERRE AND MIQUELON',\n",
    "   524 :  'ST. VINCENT-GRENADINES',\n",
    "   716 :  'SAINT BARTHELEMY' ,\n",
    "   736 :  'SAINT MARTIN' ,\n",
    "   749 :  'SAINT MAARTEN' ,\n",
    "   350 :  'SUDAN',\n",
    "   602 :  'SURINAME',\n",
    "   351 :  'SWAZILAND',\n",
    "   130 :  'SWEDEN',\n",
    "   131 :  'SWITZERLAND',\n",
    "   262 :  'SYRIA',\n",
    "   268 :  'TAIWAN',\n",
    "   159 :  'TAJIKISTAN',\n",
    "   353 :  'TANZANIA',\n",
    "   263 :  'THAILAND',\n",
    "   304 :  'TOGO',\n",
    "   417 :  'TONGA',\n",
    "   516 :  'TRINIDAD AND TOBAGO',\n",
    "   323 :  'TUNISIA',\n",
    "   264 :  'TURKEY',\n",
    "   161 :  'TURKMENISTAN',\n",
    "   527 :  'TURKS AND CAICOS ISLANDS',\n",
    "   420 :  'TUVALU',\n",
    "   352 :  'UGANDA',\n",
    "   162 :  'UKRAINE',\n",
    "   296 :  'UNITED ARAB EMIRATES',\n",
    "   135 :  'UNITED KINGDOM',\n",
    "   695 :  'URUGUAY',\n",
    "   163 :  'UZBEKISTAN',\n",
    "   410 :  'VANUATU',\n",
    "   696 :  'VENEZUELA',\n",
    "   266 :  'VIETNAM',\n",
    "   469 :  'WALLIS AND FUTUNA ISLANDS',\n",
    "   757 :  'WEST INDIES (FRENCH)' ,\n",
    "   333 :  'WESTERN SAHARA',\n",
    "   465 :  'WESTERN SAMOA',\n",
    "   216 :  'YEMEN',\n",
    "   139 :  'YUGOSLAVIA',\n",
    "   301 :  'ZAIRE',\n",
    "   344 :  'ZAMBIA',\n",
    "   315 :  'ZIMBABWE',\n",
    "   403 :  'INVALID: AMERICAN SAMOA',\n",
    "   712 :  'INVALID: ANTARCTICA' ,\n",
    "   700 :  'INVALID: BORN ON BOARD SHIP',\n",
    "   719 :  'INVALID: BOUVET ISLAND (ANTARCTICA/NORWAY TERR.)',\n",
    "   574 :  'INVALID: CANADA',\n",
    "   720 :  'INVALID: CANTON AND ENDERBURY ISLS' ,\n",
    "   106 :  'INVALID: CZECHOSLOVAKIA',\n",
    "   739 :  'INVALID: DRONNING MAUD LAND (ANTARCTICA-NORWAY)' ,\n",
    "   394 :  'INVALID: FRENCH SOUTHERN AND ANTARCTIC',\n",
    "   501 :  'INVALID: GREENLAND',\n",
    "   404 :  'INVALID: GUAM',\n",
    "   730 :  'INVALID: INTERNATIONAL WATERS' ,\n",
    "   731 :  'INVALID: JOHNSON ISLAND' ,\n",
    "   471 :  'INVALID: MARIANA ISLANDS, NORTHERN',\n",
    "   737 :  'INVALID: MIDWAY ISLANDS' ,\n",
    "   753 :  'INVALID: MINOR OUTLYING ISLANDS - USA',\n",
    "   740 :  'INVALID: NEUTRAL ZONE (S. ARABIA/IRAQ)' ,\n",
    "   710 :  'INVALID: NON-QUOTA IMMIGRANT',\n",
    "   505 :  'INVALID: PUERTO RICO',\n",
    "    0  :  'INVALID: STATELESS',\n",
    "   705 :  'INVALID: STATELESS',\n",
    "   583 :  'INVALID: UNITED STATES',\n",
    "   407 :  'INVALID: UNITED STATES',\n",
    "   999 :  'INVALID: UNKNOWN',\n",
    "   239 :  'INVALID: UNKNOWN COUNTRY',\n",
    "   134 :  'INVALID: USSR',\n",
    "   506 :  'INVALID: U.S. VIRGIN ISLANDS',\n",
    "   755 :  'INVALID: WAKE ISLAND'  ,\n",
    "   311 :  'Collapsed Tanzania (should not show)',\n",
    "   741 :  'Collapsed Curacao (should not show)',\n",
    "    54 :  'No Country Code (54)',\n",
    "   100 :  'No Country Code (100)',\n",
    "   187 :  'No Country Code (187)',\n",
    "   190 :  'No Country Code (190)',\n",
    "   200 :  'No Country Code (200)',\n",
    "   219 :  'No Country Code (219)',\n",
    "   238 :  'No Country Code (238)',\n",
    "   277 :  'No Country Code (277)',\n",
    "   293 :  'No Country Code (293)',\n",
    "   300 :  'No Country Code (300)',\n",
    "   319 :  'No Country Code (319)',\n",
    "   365 :  'No Country Code (365)',\n",
    "   395 :  'No Country Code (395)',\n",
    "   400 :  'No Country Code (400)',\n",
    "   485 :  'No Country Code (485)',\n",
    "   503 :  'No Country Code (503)',\n",
    "   589 :  'No Country Code (589)',\n",
    "   592 :  'No Country Code (592)',\n",
    "   791 :  'No Country Code (791)',\n",
    "   849 :  'No Country Code (849)',\n",
    "   914 :  'No Country Code (914)',\n",
    "   944 :  'No Country Code (944)',\n",
    "   996 :  'No Country Code (996)'\n",
    "}\n",
    "\n",
    "df_city_res_codes = spark.createDataFrame(list(map(list, cit_and_res_codes.items())),\n",
    "                                        [\"country_code\",\"country\"])\n",
    "df_city_res_codes.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Travel mode Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travel_mode_code</th>\n",
       "      <th>travel_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not reported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   travel_mode_code   travel_code\n",
       "0                 1           Air\n",
       "1                 2           Sea\n",
       "2                 3          Land\n",
       "3                 9  Not reported"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_codes = {\n",
    "    1 : 'Air',\n",
    "    2 : 'Sea',\n",
    "    3 : 'Land',\n",
    "    9 : 'Not reported'\n",
    "}\n",
    "df_travel_mode = spark.createDataFrame(list(map(list, mode_codes.items())),\n",
    "                                        [\"travel_mode_code\",\"travel_code\"])\n",
    "df_travel_mode = df_travel_mode.withColumn(\"travel_mode_code\",df_travel_mode[\"travel_mode_code\"].cast(IntegerType()))\n",
    "df_travel_mode.toPandas()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "State Code Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_code       state\n",
       "0         AL     ALABAMA\n",
       "1         AK      ALASKA\n",
       "2         AZ     ARIZONA\n",
       "3         AR    ARKANSAS\n",
       "4         CA  CALIFORNIA"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_codes = {\n",
    "    'AL' : 'ALABAMA',\n",
    "    'AK' : 'ALASKA',\n",
    "    'AZ' : 'ARIZONA',\n",
    "    'AR' : 'ARKANSAS',\n",
    "    'CA' : 'CALIFORNIA',\n",
    "    'CO' : 'COLORADO',\n",
    "    'CT' : 'CONNECTICUT',\n",
    "    'DE' : 'DELAWARE',\n",
    "    'DC' :'DIST. OF COLUMBIA',\n",
    "    'FL' :'FLORIDA',\n",
    "    'GA' :'GEORGIA',\n",
    "    'GU' :'GUAM',\n",
    "    'HI' :'HAWAII',\n",
    "    'ID' :'IDAHO',\n",
    "    'IL' :'ILLINOIS',\n",
    "    'IN' :'INDIANA',\n",
    "    'IA' :'IOWA',\n",
    "    'KS' :'KANSAS',\n",
    "    'KY' :'KENTUCKY',\n",
    "    'LA' :'LOUISIANA',\n",
    "    'ME' :'MAINE',\n",
    "    'MD' :'MARYLAND',\n",
    "    'MA' :'MASSACHUSETTS',\n",
    "    'MI' :'MICHIGAN',\n",
    "    'MN' :'MINNESOTA',\n",
    "    'MS' :'MISSISSIPPI',\n",
    "    'MO' :'MISSOURI',\n",
    "    'MT' :'MONTANA',\n",
    "    'NC' :'N. CAROLINA',\n",
    "    'ND' :'N. DAKOTA',\n",
    "    'NE' :'NEBRASKA',\n",
    "    'NV' :'NEVADA',\n",
    "    'NH' :'NEW HAMPSHIRE',\n",
    "    'NJ' :'NEW JERSEY',\n",
    "    'NM' :'NEW MEXICO',\n",
    "    'NY' :'NEW YORK',\n",
    "    'OH' :'OHIO',\n",
    "    'OK' :'OKLAHOMA',\n",
    "    'OR' :'OREGON',\n",
    "    'PA' :'PENNSYLVANIA',\n",
    "    'PR' :'PUERTO RICO',\n",
    "    'RI' :'RHODE ISLAND',\n",
    "    'SC' :'S. CAROLINA',\n",
    "    'SD' :'S. DAKOTA',\n",
    "    'TN' :'TENNESSEE',\n",
    "    'TX' :'TEXAS',\n",
    "    'UT' :'UTAH',\n",
    "    'VT' :'VERMONT',\n",
    "    'VI' :'VIRGIN ISLANDS',\n",
    "    'VA' :'VIRGINIA',\n",
    "    'WV' :'W. VIRGINIA',\n",
    "    'WA' :'WASHINGTON',\n",
    "    'WI' :'WISCONSON',\n",
    "    'WY' :'WYOMING' ,\n",
    "    '99' :'All Other Codes' \n",
    "}\n",
    "df_state_codes = spark.createDataFrame(list(map(list, state_codes.items())),\n",
    "                                        [\"state_code\",\"state\"])\n",
    "df_state_codes.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Port Code Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_code</th>\n",
       "      <th>port_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW, AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  port_code                     port_name\n",
       "0       ALC        ALCAN, AK             \n",
       "1       ANC        ANCHORAGE, AK         \n",
       "2       BAR  BAKER AAF - BAKER ISLAND, AK\n",
       "3       DAC        DALTONS CACHE, AK     \n",
       "4       PIZ    DEW STATION PT LAY DEW, AK"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ports_codes = {\n",
    "   'ALC'    :    'ALCAN, AK             ',\n",
    "   'ANC'    :    'ANCHORAGE, AK         ',\n",
    "   'BAR'    :    'BAKER AAF - BAKER ISLAND, AK',\n",
    "   'DAC'    :    'DALTONS CACHE, AK     ',\n",
    "   'PIZ'    :    'DEW STATION PT LAY DEW, AK',\n",
    "   'DTH'    :    'DUTCH HARBOR, AK      ',\n",
    "   'EGL'    :    'EAGLE, AK             ',\n",
    "   'FRB'    :    'FAIRBANKS, AK         ',\n",
    "   'HOM'    :    'HOMER, AK             '           ,\n",
    "   'HYD'    :    'HYDER, AK             ',\n",
    "   'JUN'    :    'JUNEAU, AK            ',\n",
    "   '5KE'    :    'KETCHIKAN, AK',\n",
    "   'KET'    :    'KETCHIKAN, AK         ',\n",
    "   'MOS'    :    'MOSES POINT INTERMEDIATE, AK',\n",
    "   'NIK'    :    'NIKISKI, AK           ',\n",
    "   'NOM'    :    'NOM, AK               ',\n",
    "   'PKC'    :    'POKER CREEK, AK       ',\n",
    "   'ORI'    :    'PORT LIONS SPB, AK',\n",
    "   'SKA'    :    'SKAGWAY, AK           ',\n",
    "   'SNP'    :    'ST. PAUL ISLAND, AK',\n",
    "   'TKI'    :    'TOKEEN, AK',\n",
    "   'WRA'    :    'WRANGELL, AK          ',\n",
    "   'HSV'    :    'MADISON COUNTY - HUNTSVILLE, AL',\n",
    "   'MOB'    :    'MOBILE, AL            ',\n",
    "   'LIA'    :    'LITTLE ROCK, AR (BPS)',\n",
    "   'ROG'    :    'ROGERS ARPT, AR',\n",
    "   'DOU'    :    'DOUGLAS, AZ           ',\n",
    "   'LUK'    :    'LUKEVILLE, AZ         ',\n",
    "   'MAP'    :    'MARIPOSA AZ           ',\n",
    "   'NAC'    :    'NACO, AZ              ',\n",
    "   'NOG'    :    'NOGALES, AZ           ',\n",
    "   'PHO'    :    'PHOENIX, AZ           ',\n",
    "   'POR'    :    'PORTAL, AZ',\n",
    "   'SLU'    :    'SAN LUIS, AZ          ',\n",
    "   'SAS'    :    'SASABE, AZ            ',\n",
    "   'TUC'    :    'TUCSON, AZ            ',\n",
    "   'YUI'    :    'YUMA, AZ              ' ,\n",
    "   'AND'    :    'ANDRADE, CA           ',\n",
    "   'BUR'    :    'BURBANK, CA',\n",
    "   'CAL'    :    'CALEXICO, CA          ',\n",
    "   'CAO'    :    'CAMPO, CA             ' ,\n",
    "   'FRE'    :    'FRESNO, CA            ',\n",
    "   'ICP'    :    'IMPERIAL COUNTY, CA   ',\n",
    "   'LNB'    :    'LONG BEACH, CA         ',\n",
    "   'LOS'    :    'LOS ANGELES, CA       ',\n",
    "   'BFL'    :    'MEADOWS FIELD - BAKERSFIELD, CA',\n",
    "   'OAK'    :    'OAKLAND, CA ' ,\n",
    "   'ONT'    :    'ONTARIO, CA',\n",
    "   'OTM'    :    'OTAY MESA, CA          ',\n",
    "   'BLT'    :    'PACIFIC, HWY. STATION, CA ',\n",
    "   'PSP'    :    'PALM SPRINGS, CA',\n",
    "   'SAC'    :    'SACRAMENTO, CA        ',\n",
    "   'SLS'    :    'SALINAS, CA (BPS)',\n",
    "   'SDP'    :    'SAN DIEGO, CA',\n",
    "   'SFR'    :    'SAN FRANCISCO, CA     ',\n",
    "   'SNJ'    :    'SAN JOSE, CA          ',\n",
    "   'SLO'    :    'SAN LUIS OBISPO, CA   ',\n",
    "   'SLI'    :    'SAN LUIS OBISPO, CA (BPS)',\n",
    "   'SPC'    :    'SAN PEDRO, CA         ',\n",
    "   'SYS'    :    'SAN YSIDRO, CA        ',\n",
    "   'SAA'    :    'SANTA ANA, CA         ',\n",
    "   'STO'    :    'STOCKTON, CA (BPS)',\n",
    "   'TEC'    :    'TECATE, CA            ',\n",
    "   'TRV'    :    'TRAVIS-AFB, CA        ',\n",
    "   'APA'    :    'ARAPAHOE COUNTY, CO',\n",
    "   'ASE'    :    'ASPEN, CO #ARPT',\n",
    "   'COS'    :    'COLORADO SPRINGS, CO',\n",
    "   'DEN'    :    'DENVER, CO            ',\n",
    "   'DRO'    :    'LA PLATA - DURANGO, CO',\n",
    "   'BDL'    :    'BRADLEY INTERNATIONAL, CT',\n",
    "   'BGC'    :    'BRIDGEPORT, CT        ',\n",
    "   'GRT'    :    'GROTON, CT            ',\n",
    "   'HAR'    :    'HARTFORD, CT          ',\n",
    "   'NWH'    :    'NEW HAVEN, CT         ',\n",
    "   'NWL'    :    'NEW LONDON, CT        ',\n",
    "   'TST'    :    'NEWINGTON DATA CENTER TEST, CT',\n",
    "   'WAS'    :    'WASHINGTON DC         ',\n",
    "   'DOV'    :    'DOVER AFB, DE',\n",
    "   'DVD'    :    'DOVER-AFB, DE         ',\n",
    "   'WLL'    :    'WILMINGTON, DE        ',\n",
    "   'BOC'    :    'BOCAGRANDE, FL        ',\n",
    "   'SRQ'    :    'BRADENTON - SARASOTA, FL',\n",
    "   'CAN'    :    'CAPE CANAVERAL, FL    ',\n",
    "   'DAB'    :    'DAYTONA BEACH INTERNATIONAL, FL',\n",
    "   'FRN'    :    'FERNANDINA, FL        ',\n",
    "   'FTL'    :    'FORT LAUDERDALE, FL   ',\n",
    "   'FMY'    :    'FORT MYERS, FL        ',\n",
    "   'FPF'    :    'FORT PIERCE, FL       ',\n",
    "   'HUR'    :    'HURLBURT FIELD, FL',\n",
    "   'GNV'    :    'J R ALISON MUNI - GAINESVILLE, FL',\n",
    "   'JAC'    :    'JACKSONVILLE, FL      ',\n",
    "   'KEY'    :    'KEY WEST, FL          ',\n",
    "   'LEE'    :    'LEESBURG MUNICIPAL AIRPORT, FL',\n",
    "   'MLB'    :    'MELBOURNE, FL',\n",
    "   'MIA'    :    'MIAMI, FL             ',\n",
    "   'APF'    :    'NAPLES, FL #ARPT',\n",
    "   'OPF'    :    'OPA LOCKA, FL',\n",
    "   'ORL'    :    'ORLANDO, FL           ',\n",
    "   'PAN'    :    'PANAMA CITY, FL       ',\n",
    "   'PEN'    :    'PENSACOLA, FL         ',\n",
    "   'PCF'    :    'PORT CANAVERAL, FL    ',\n",
    "   'PEV'    :    'PORT EVERGLADES, FL   ',\n",
    "   'PSJ'    :    'PORT ST JOE, FL       ',\n",
    "   'SFB'    :    'SANFORD, FL           ',\n",
    "   'SGJ'    :    'ST AUGUSTINE ARPT, FL',\n",
    "   'SAU'    :    'ST AUGUSTINE, FL      ',\n",
    "   'FPR'    :    'ST LUCIE COUNTY, FL',\n",
    "   'SPE'    :    'ST PETERSBURG, FL     ',\n",
    "   'TAM'    :    'TAMPA, FL             ',\n",
    "   'WPB'    :    'WEST PALM BEACH, FL   ',\n",
    "   'ATL'    :    'ATLANTA, GA           ',\n",
    "   'BRU'    :    'BRUNSWICK, GA         ',\n",
    "   'AGS'    :    'BUSH FIELD - AUGUSTA, GA',\n",
    "   'SAV'    :    'SAVANNAH, GA          ',\n",
    "   'AGA'    :    'AGANA, GU             ',\n",
    "   'HHW'    :    'HONOLULU, HI          ',\n",
    "   'OGG'    :    'KAHULUI - MAUI, HI',\n",
    "   'KOA'    :    'KEAHOLE-KONA, HI      ',\n",
    "   'LIH'    :    'LIHUE, HI             ',\n",
    "   'CID'    :    'CEDAR RAPIDS/IOWA CITY, IA',\n",
    "   'DSM'    :    'DES MOINES, IA',\n",
    "   'BOI'    :    'AIR TERM. (GOWEN FLD) BOISE, ID',\n",
    "   'EPI'    :    'EASTPORT, ID          ',\n",
    "   'IDA'    :    'FANNING FIELD - IDAHO FALLS, ID',\n",
    "   'PTL'    :    'PORTHILL, ID          ',\n",
    "   'SPI'    :    'CAPITAL - SPRINGFIELD, IL',\n",
    "   'CHI'    :    'CHICAGO, IL           ',\n",
    "   'DPA'    :    'DUPAGE COUNTY, IL',\n",
    "   'PIA'    :    'GREATER PEORIA, IL',\n",
    "   'RFD'    :    'GREATER ROCKFORD, IL',\n",
    "   'UGN'    :    'MEMORIAL - WAUKEGAN, IL',\n",
    "   'GAR'    :    'GARY, IN              ',\n",
    "   'HMM'    :    'HAMMOND, IN           ',\n",
    "   'INP'    :    'INDIANAPOLIS, IN      ',\n",
    "   'MRL'    :    'MERRILLVILLE, IN      ',\n",
    "   'SBN'    :    'SOUTH BEND, IN',\n",
    "   'ICT'    :    'MID-CONTINENT - WITCHITA, KS',\n",
    "   'LEX'    :    'BLUE GRASS - LEXINGTON, KY',\n",
    "   'LOU'    :    'LOUISVILLE, KY        ',\n",
    "   'BTN'    :    'BATON ROUGE, LA       ',\n",
    "   'LKC'    :    'LAKE CHARLES, LA      ',\n",
    "   'LAK'    :    'LAKE CHARLES, LA (BPS)',\n",
    "   'MLU'    :    'MONROE, LA',\n",
    "   'MGC'    :    'MORGAN CITY, LA       ',\n",
    "   'NOL'    :    'NEW ORLEANS, LA       ',\n",
    "   'BOS'    :    'BOSTON, MA            ',\n",
    "   'GLO'    :    'GLOUCESTER, MA        ',\n",
    "   'BED'    :    'HANSCOM FIELD - BEDFORD, MA',\n",
    "   'LYN'    :    'LYNDEN, WA            ',\n",
    "   'ADW'    :    'ANDREWS AFB, MD',\n",
    "   'BAL'    :    'BALTIMORE, MD         ',\n",
    "   'MKG'    :    'MUSKEGON, MD',\n",
    "   'PAX'    :    'PATUXENT RIVER, MD    ',\n",
    "   'BGM'    :    'BANGOR, ME            ',\n",
    "   'BOO'    :    'BOOTHBAY HARBOR, ME   ',\n",
    "   'BWM'    :    'BRIDGEWATER, ME       ',\n",
    "   'BCK'    :    'BUCKPORT, ME          ',\n",
    "   'CLS'    :    'CALAIS, ME   ',\n",
    "   'CRB'    :    'CARIBOU, ME           ',\n",
    "   'COB'    :    'COBURN GORE, ME       ',\n",
    "   'EST'    :    'EASTCOURT, ME         ',\n",
    "   'EPT'    :    'EASTPORT MUNICIPAL, ME',\n",
    "   'EPM'    :    'EASTPORT, ME          ',\n",
    "   'FOR'    :    'FOREST CITY, ME       ',\n",
    "   'FTF'    :    'FORT FAIRFIELD, ME    ',\n",
    "   'FTK'    :    'FORT KENT, ME         ',\n",
    "   'HML'    :    'HAMIIN, ME            ',\n",
    "   'HTM'    :    'HOULTON, ME           ',\n",
    "   'JKM'    :    'JACKMAN, ME           ',\n",
    "   'KAL'    :    'KALISPEL, MT          ',\n",
    "   'LIM'    :    'LIMESTONE, ME         ',\n",
    "   'LUB'    :    'LUBEC, ME             ',\n",
    "   'MAD'    :    'MADAWASKA, ME         ',\n",
    "   'POM'    :    'PORTLAND, ME          ',\n",
    "   'RGM'    :    'RANGELEY, ME (BPS)',\n",
    "   'SBR'    :    'SOUTH BREWER, ME      ',\n",
    "   'SRL'    :    'ST AURELIE, ME        ',\n",
    "   'SPA'    :    'ST PAMPILE, ME        ',\n",
    "   'VNB'    :    'VAN BUREN, ME         ',\n",
    "   'VCB'    :    'VANCEBORO, ME         ',\n",
    "   'AGN'    :    'ALGONAC, MI           ',\n",
    "   'ALP'    :    'ALPENA, MI            ',\n",
    "   'BCY'    :    'BAY CITY, MI          ',\n",
    "   'DET'    :    'DETROIT, MI           ',\n",
    "   'GRP'    :    'GRAND RAPIDS, MI',\n",
    "   'GRO'    :    'GROSSE ISLE, MI       ',\n",
    "   'ISL'    :    'ISLE ROYALE, MI       ',\n",
    "   'MRC'    :    'MARINE CITY, MI       ',\n",
    "   'MRY'    :    'MARYSVILLE, MI        ',\n",
    "   'PTK'    :    'OAKLAND COUNTY - PONTIAC, MI',\n",
    "   'PHU'    :    'PORT HURON, MI        ',\n",
    "   'RBT'    :    'ROBERTS LANDING, MI   ',\n",
    "   'SAG'    :    'SAGINAW, MI           ',\n",
    "   'SSM'    :    'SAULT STE. MARIE, MI  ',\n",
    "   'SCL'    :    'ST CLAIR, MI          ',\n",
    "   'YIP'    :    'WILLOW RUN - YPSILANTI, MI',\n",
    "   'BAU'    :    'BAUDETTE, MN          ',\n",
    "   'CAR'    :    'CARIBOU MUNICIPAL AIRPORT, MN',\n",
    "   'GTF'    :    'Collapsed into INT, MN',\n",
    "   'INL'    :    'Collapsed into INT, MN',\n",
    "   'CRA'    :    'CRANE LAKE, MN        ',\n",
    "   'MIC'    :    'CRYSTAL MUNICIPAL AIRPORT, MN',\n",
    "   'DUL'    :    'DULUTH, MN            ',\n",
    "   'ELY'    :    'ELY, MN               ',\n",
    "   'GPM'    :    'GRAND PORTAGE, MN     ',\n",
    "   'SVC'    :    'GRANT COUNTY - SILVER CITY, MN',\n",
    "   'INT'    :    'INT''L FALLS, MN      ',\n",
    "   'LAN'    :    'LANCASTER, MN         ',\n",
    "   'MSP'    :    'MINN./ST PAUL, MN     ',\n",
    "   'LIN'    :    'NORTHERN SVC CENTER, MN   ',\n",
    "   'NOY'    :    'NOYES, MN             ',\n",
    "   'PIN'    :    'PINE CREEK, MN        ',\n",
    "   '48Y'    :    'PINECREEK BORDER ARPT, MN',\n",
    "   'RAN'    :    'RAINER, MN            ',\n",
    "   'RST'    :    'ROCHESTER, MN',\n",
    "   'ROS'    :    'ROSEAU, MN            ',\n",
    "   'SPM'    :    'ST PAUL, MN           ',\n",
    "   'WSB'    :    'WARROAD INTL, SPB, MN',\n",
    "   'WAR'    :    'WARROAD, MN           ',\n",
    "   'KAN'    :    'KANSAS CITY, MO       ',\n",
    "   'SGF'    :    'SPRINGFIELD-BRANSON, MO',\n",
    "   'STL'    :    'ST LOUIS, MO          ',\n",
    "   'WHI'    :    'WHITETAIL, MT         ',\n",
    "   'WHM'    :    'WILD HORSE, MT        ',\n",
    "   'GPT'    :    'BILOXI REGIONAL, MS',\n",
    "   'GTR'    :    'GOLDEN TRIANGLE LOWNDES CNTY, MS',\n",
    "   'GUL'    :    'GULFPORT, MS          ',\n",
    "   'PAS'    :    'PASCAGOULA, MS        ',\n",
    "   'JAN'    :    'THOMPSON FIELD - JACKSON, MS',\n",
    "   'BIL'    :    'BILLINGS, MT          ',\n",
    "   'BTM'    :    'BUTTE, MT             ',\n",
    "   'CHF'    :    'CHIEF MT, MT          ',\n",
    "   'CTB'    :    'CUT BANK MUNICIPAL, MT',\n",
    "   'CUT'    :    'CUT BANK, MT          ',\n",
    "   'DLB'    :    'DEL BONITA, MT        ',\n",
    "   'EUR'    :    'EUREKA, MT (BPS)',\n",
    "   'BZN'    :    'GALLATIN FIELD - BOZEMAN, MT',\n",
    "   'FCA'    :    'GLACIER NATIONAL PARK, MT',\n",
    "   'GGW'    :    'GLASGOW, MT           ',\n",
    "   'GRE'    :    'GREAT FALLS, MT       ',\n",
    "   'HVR'    :    'HAVRE, MT             ',\n",
    "   'HEL'    :    'HELENA, MT            ',\n",
    "   'LWT'    :    'LEWISTON, MT          ',\n",
    "   'MGM'    :    'MORGAN, MT            ',\n",
    "   'OPH'    :    'OPHEIM, MT            ',\n",
    "   'PIE'    :    'PIEGAN, MT            ',\n",
    "   'RAY'    :    'RAYMOND, MT           ',\n",
    "   'ROO'    :    'ROOSVILLE, MT         ',\n",
    "   'SCO'    :    'SCOBEY, MT            ',\n",
    "   'SWE'    :    'SWEETGTASS, MT        ',\n",
    "   'TRL'    :    'TRIAL CREEK, MT       ',\n",
    "   'TUR'    :    'TURNER, MT            ',\n",
    "   'WCM'    :    'WILLOW CREEK, MT      ',\n",
    "   'CLT'    :    'CHARLOTTE, NC         ',\n",
    "   'FAY'    :    'FAYETTEVILLE, NC',\n",
    "   'MRH'    :    'MOREHEAD CITY, NC     ',\n",
    "   'FOP'    :    'MORRIS FIELDS AAF, NC',\n",
    "   'GSO'    :    'PIEDMONT TRIAD INTL AIRPORT, NC',\n",
    "   'RDU'    :    'RALEIGH/DURHAM, NC    ',\n",
    "   'SSC'    :    'SHAW AFB - SUMTER, NC',\n",
    "   'WIL'    :    'WILMINGTON, NC        ',\n",
    "   'AMB'    :    'AMBROSE, ND           ',\n",
    "   'ANT'    :    'ANTLER, ND            ',\n",
    "   'CRY'    :    'CARBURY, ND           ',\n",
    "   'DNS'    :    'DUNSEITH, ND          ',\n",
    "   'FAR'    :    'FARGO, ND             ',\n",
    "   'FRT'    :    'FORTUNA, ND           ',\n",
    "   'GRF'    :    'GRAND FORKS, ND       ',\n",
    "   'HNN'    :    'HANNAH, ND            ',\n",
    "   'HNS'    :    'HANSBORO, ND          ',\n",
    "   'MAI'    :    'MAIDA, ND             ',\n",
    "   'MND'    :    'MINOT, ND             ',\n",
    "   'NEC'    :    'NECHE, ND             ',\n",
    "   'NOO'    :    'NOONAN, ND            ',\n",
    "   'NRG'    :    'NORTHGATE, ND         ',\n",
    "   'PEM'    :    'PEMBINA, ND           ',\n",
    "   'SAR'    :    'SARLES, ND            ',\n",
    "   'SHR'    :    'SHERWOOD, ND          ',\n",
    "   'SJO'    :    'ST JOHN, ND           ',\n",
    "   'WAL'    :    'WALHALLA, ND          ',\n",
    "   'WHO'    :    'WESTHOPE, ND          ',\n",
    "   'WND'    :    'WILLISTON, ND         ',\n",
    "   'OMA'    :    'OMAHA, NE             ',\n",
    "   'LEB'    :    'LEBANON, NH           ',\n",
    "   'MHT'    :    'MANCHESTER, NH',\n",
    "   'PNH'    :    'PITTSBURG, NH         ',\n",
    "   'PSM'    :    'PORTSMOUTH, NH        ',\n",
    "   'BYO'    :    'BAYONNE, NJ           ',\n",
    "   'CNJ'    :    'CAMDEN, NJ            ',\n",
    "   'HOB'    :    'HOBOKEN, NJ           ',\n",
    "   'JER'    :    'JERSEY CITY, NJ       ',\n",
    "   'WRI'    :    'MC GUIRE AFB - WRIGHTSOWN, NJ',\n",
    "   'MMU'    :    'MORRISTOWN, NJ',\n",
    "   'NEW'    :    'NEWARK/TETERBORO, NJ  ',\n",
    "   'PER'    :    'PERTH AMBOY, NJ       ',\n",
    "   'ACY'    :    'POMONA FIELD - ATLANTIC CITY, NJ',\n",
    "   'ALA'    :    'ALAMAGORDO, NM (BPS)',\n",
    "   'ABQ'    :    'ALBUQUERQUE, NM       ',\n",
    "   'ANP'    :    'ANTELOPE WELLS, NM    ',\n",
    "   'CRL'    :    'CARLSBAD, NM          ',\n",
    "   'COL'    :    'COLUMBUS, NM          ',\n",
    "   'CDD'    :    'CRANE LAKE - ST. LOUIS CNTY, NM',\n",
    "   'DNM'    :    'DEMING, NM (BPS)',\n",
    "   'LAS'    :    'LAS CRUCES, NM        ',\n",
    "   'LOB'    :    'LORDSBURG, NM (BPS)',\n",
    "   'RUI'    :    'RUIDOSO, NM',\n",
    "   'STR'    :    'SANTA TERESA, NM      ',\n",
    "   'RNO'    :    'CANNON INTL - RENO/TAHOE, NV',\n",
    "   'FLX'    :    'FALLON MUNICIPAL AIRPORT, NV',\n",
    "   'LVG'    :    'LAS VEGAS, NV         ',\n",
    "   'REN'    :    'RENO, NV              ',\n",
    "   'ALB'    :    'ALBANY, NY            ',\n",
    "   'AXB'    :    'ALEXANDRIA BAY, NY    ',\n",
    "   'BUF'    :    'BUFFALO, NY           ',\n",
    "   'CNH'    :    'CANNON CORNERS, NY',\n",
    "   'CAP'    :    'CAPE VINCENT, NY      ',\n",
    "   'CHM'    :    'CHAMPLAIN, NY         ',\n",
    "   'CHT'    :    'CHATEAUGAY, NY        ',\n",
    "   'CLA'    :    'CLAYTON, NY           ',\n",
    "   'FTC'    :    'FORT COVINGTON, NY    ',\n",
    "   'LAG'    :    'LA GUARDIA, NY        ',\n",
    "   'LEW'    :    'LEWISTON, NY          ',\n",
    "   'MAS'    :    'MASSENA, NY           ',\n",
    "   'MAG'    :    'MCGUIRE AFB, NY       ',\n",
    "   'MOO'    :    'MOORES, NY            ',\n",
    "   'MRR'    :    'MORRISTOWN, NY        ',\n",
    "   'NYC'    :    'NEW YORK, NY          ',\n",
    "   'NIA'    :    'NIAGARA FALLS, NY     ',\n",
    "   'OGD'    :    'OGDENSBURG, NY        ',\n",
    "   'OSW'    :    'OSWEGO, NY            ',\n",
    "   'ELM'    :    'REGIONAL ARPT - HORSEHEAD, NY',\n",
    "   'ROC'    :    'ROCHESTER, NY         ',\n",
    "   'ROU'    :    'ROUSES POINT, NY      ',\n",
    "   'SWF'    :    'STEWART - ORANGE CNTY, NY',\n",
    "   'SYR'    :    'SYRACUSE, NY          ',\n",
    "   'THO'    :    'THOUSAND ISLAND BRIDGE, NY',\n",
    "   'TRO'    :    'TROUT RIVER, NY       ',\n",
    "   'WAT'    :    'WATERTOWN, NY         ',\n",
    "   'HPN'    :    'WESTCHESTER - WHITE PLAINS, NY',\n",
    "   'WRB'    :    'WHIRLPOOL BRIDGE, NY',\n",
    "   'YOU'    :    'YOUNGSTOWN, NY        ',\n",
    "   'AKR'    :    'AKRON, OH             ',\n",
    "   'ATB'    :    'ASHTABULA, OH         ',\n",
    "   'CIN'    :    'CINCINNATI, OH        ',\n",
    "   'CLE'    :    'CLEVELAND, OH         ',\n",
    "   'CLM'    :    'COLUMBUS, OH          ',\n",
    "   'LOR'    :    'LORAIN, OH            ',\n",
    "   'MBO'    :    'MARBLE HEADS, OH      ',\n",
    "   'SDY'    :    'SANDUSKY, OH          ',\n",
    "   'TOL'    :    'TOLEDO, OH            ',\n",
    "   'OKC'    :    'OKLAHOMA CITY, OK     ',\n",
    "   'TUL'    :    'TULSA, OK',\n",
    "   'AST'    :    'ASTORIA, OR           ',\n",
    "   'COO'    :    'COOS BAY, OR          ',\n",
    "   'HIO'    :    'HILLSBORO, OR',\n",
    "   'MED'    :    'MEDFORD, OR           ',\n",
    "   'NPT'    :    'NEWPORT, OR           ',\n",
    "   'POO'    :    'PORTLAND, OR          ',\n",
    "   'PUT'    :    'PUT-IN-BAY, OH        ',\n",
    "   'RDM'    :    'ROBERTS FIELDS - REDMOND, OR',\n",
    "   'ERI'    :    'ERIE, PA              ',\n",
    "   'MDT'    :    'HARRISBURG, PA',\n",
    "   'HSB'    :    'HARRISONBURG, PA      ',\n",
    "   'PHI'    :    'PHILADELPHIA, PA      ',\n",
    "   'PIT'    :    'PITTSBURG, PA         ',\n",
    "   'AGU'    :    'AGUADILLA, PR         ',\n",
    "   'BQN'    :    'BORINQUEN - AGUADILLO, PR',\n",
    "   'JCP'    :    'CULEBRA - BENJAMIN RIVERA, PR',\n",
    "   'ENS'    :    'ENSENADA, PR          ',\n",
    "   'FAJ'    :    'FAJARDO, PR           ',\n",
    "   'HUM'    :    'HUMACAO, PR           ',\n",
    "   'JOB'    :    'JOBOS, PR             ',\n",
    "   'MAY'    :    'MAYAGUEZ, PR          ',\n",
    "   'PON'    :    'PONCE, PR             ',\n",
    "   'PSE'    :    'PONCE-MERCEDITA, PR',\n",
    "   'SAJ'    :    'SAN JUAN, PR          ',\n",
    "   'VQS'    :    'VIEQUES-ARPT, PR',\n",
    "   'PRO'    :    'PROVIDENCE, RI        ',\n",
    "   'PVD'    :    'THEODORE FRANCIS - WARWICK, RI',\n",
    "   'CHL'    :    'CHARLESTON, SC        ',\n",
    "   'CAE'    :    'COLUMBIA, SC #ARPT',\n",
    "   'GEO'    :    'GEORGETOWN, SC        ',\n",
    "   'GSP'    :    'GREENVILLE, SC',\n",
    "   'GRR'    :    'GREER, SC',\n",
    "   'MYR'    :    'MYRTLE BEACH, SC',\n",
    "   'SPF'    :    'BLACK HILLS, SPEARFISH, SD',\n",
    "   'HON'    :    'HOWES REGIONAL ARPT - HURON, SD',\n",
    "   'SAI'    :    'SAIPAN, SPN           ',\n",
    "   'TYS'    :    'MC GHEE TYSON - ALCOA, TN',\n",
    "   'MEM'    :    'MEMPHIS, TN           ',\n",
    "   'NSV'    :    'NASHVILLE, TN         ',\n",
    "   'TRI'    :    'TRI CITY ARPT, TN',\n",
    "   'ADS'    :    'ADDISON AIRPORT- ADDISON, TX',\n",
    "   'ADT'    :    'AMISTAD DAM, TX       ',\n",
    "   'ANZ'    :    'ANZALDUAS, TX',\n",
    "   'AUS'    :    'AUSTIN, TX            ',\n",
    "   'BEA'    :    'BEAUMONT, TX          ',\n",
    "   'BBP'    :    'BIG BEND PARK, TX (BPS)',\n",
    "   'SCC'    :    'BP SPEC COORD. CTR, TX',\n",
    "   'BTC'    :    'BP TACTICAL UNIT, TX  ' ,\n",
    "   'BOA'    :    'BRIDGE OF AMERICAS, TX',\n",
    "   'BRO'    :    'BROWNSVILLE, TX       ',\n",
    "   'CRP'    :    'CORPUS CHRISTI, TX    ',\n",
    "   'DAL'    :    'DALLAS, TX            ',\n",
    "   'DLR'    :    'DEL RIO, TX           ',\n",
    "   'DNA'    :    'DONNA, TX',\n",
    "   'EGP'    :    'EAGLE PASS, TX        ',\n",
    "   'ELP'    :    'EL PASO, TX           ',\n",
    "   'FAB'    :    'FABENS, TX            ',\n",
    "   'FAL'    :    'FALCON HEIGHTS, TX    ',\n",
    "   'FTH'    :    'FORT HANCOCK, TX      ',\n",
    "   'AFW'    :    'FORT WORTH ALLIANCE, TX',\n",
    "   'FPT'    :    'FREEPORT, TX          ',\n",
    "   'GAL'    :    'GALVESTON, TX         ',\n",
    "   'HLG'    :    'HARLINGEN, TX         ',\n",
    "   'HID'    :    'HIDALGO, TX           ',\n",
    "   'HOU'    :    'HOUSTON, TX           ',\n",
    "   'SGR'    :    'HULL FIELD, SUGAR LAND ARPT, TX',\n",
    "   'LLB'    :    'JUAREZ-LINCOLN BRIDGE, TX',\n",
    "   'LCB'    :    'LAREDO COLUMBIA BRIDGE, TX',\n",
    "   'LRN'    :    'LAREDO NORTH, TX      ',\n",
    "   'LAR'    :    'LAREDO, TX            ',\n",
    "   'LSE'    :    'LOS EBANOS, TX        ',\n",
    "   'IND'    :    'LOS INDIOS, TX',\n",
    "   'LOI'    :    'LOS INDIOS, TX        ',\n",
    "   'MRS'    :    'MARFA, TX (BPS)',\n",
    "   'MCA'    :    'MCALLEN, TX           ',\n",
    "   'MAF'    :    'ODESSA REGIONAL, TX',\n",
    "   'PDN'    :    'PASO DEL NORTE,TX     ',\n",
    "   'PBB'    :    'PEACE BRIDGE, NY      ',\n",
    "   'PHR'    :    'PHARR, TX             ',\n",
    "   'PAR'    :    'PORT ARTHUR, TX       ',\n",
    "   'ISB'    :    'PORT ISABEL, TX       ',\n",
    "   'POE'    :    'PORT OF EL PASO, TX   ',\n",
    "   'PRE'    :    'PRESIDIO, TX          ',\n",
    "   'PGR'    :    'PROGRESO, TX          ',\n",
    "   'RIO'    :    'RIO GRANDE CITY, TX   ',\n",
    "   'ROM'    :    'ROMA, TX              ',\n",
    "   'SNA'    :    'SAN ANTONIO, TX       ',\n",
    "   'SNN'    :    'SANDERSON, TX         ',\n",
    "   'VIB'    :    'VETERAN INTL BRIDGE, TX',\n",
    "   'YSL'    :    'YSLETA, TX            ',\n",
    "   'CHA'    :    'CHARLOTTE AMALIE, VI  ',\n",
    "   'CHR'    :    'CHRISTIANSTED, VI     ',\n",
    "   'CRU'    :    'CRUZ BAY, ST JOHN, VI ',\n",
    "   'FRK'    :    'FREDERIKSTED, VI      ',\n",
    "   'STT'    :    'ST THOMAS, VI         ',\n",
    "   'LGU'    :    'CACHE AIRPORT - LOGAN, UT',\n",
    "   'SLC'    :    'SALT LAKE CITY, UT    ',\n",
    "   'CHO'    :    'ALBEMARLE CHARLOTTESVILLE, VA',\n",
    "   'DAA'    :    'DAVISON AAF - FAIRFAX CNTY, VA',\n",
    "   'HOP'    :    'HOPEWELL, VA          ',\n",
    "   'HEF'    :    'MANASSAS, VA #ARPT',\n",
    "   'NWN'    :    'NEWPORT, VA           ',\n",
    "   'NOR'    :    'NORFOLK, VA           ',\n",
    "   'RCM'    :    'RICHMOND, VA          ',\n",
    "   'ABS'    :    'ALBURG SPRINGS, VT    ',\n",
    "   'ABG'    :    'ALBURG, VT            ',\n",
    "   'BEB'    :    'BEEBE PLAIN, VT       ',\n",
    "   'BEE'    :    'BEECHER FALLS, VT     ',\n",
    "   'BRG'    :    'BURLINGTON, VT        ',\n",
    "   'CNA'    :    'CANAAN, VT            ',\n",
    "   'DER'    :    'DERBY LINE, VT (I-91) ',\n",
    "   'DLV'    :    'DERBY LINE, VT (RT. 5)',\n",
    "   'ERC'    :    'EAST RICHFORD, VT     ',\n",
    "   'HIG'    :    'HIGHGATE SPRINGS, VT  ',\n",
    "   'MOR'    :    'MORSES LINE, VT       ',\n",
    "   'NPV'    :    'NEWPORT, VT           ',\n",
    "   'NRT'    :    'NORTH TROY, VT        ',\n",
    "   'NRN'    :    'NORTON, VT            ',\n",
    "   'PIV'    :    'PINNACLE ROAD, VT     ',\n",
    "   'RIF'    :    'RICHFORT, VT          ',\n",
    "   'STA'    :    'ST ALBANS, VT         ',\n",
    "   'SWB'    :    'SWANTON, VT (BP - SECTOR HQ)',\n",
    "   'WBE'    :    'WEST BERKSHIRE, VT    ',\n",
    "   'ABE'    :    'ABERDEEN, WA          ',\n",
    "   'ANA'    :    'ANACORTES, WA         ',\n",
    "   'BEL'    :    'BELLINGHAM, WA        ',\n",
    "   'BLI'    :    'BELLINGHAM, WASHINGTON #INTL',\n",
    "   'BLA'    :    'BLAINE, WA            ',\n",
    "   'BWA'    :    'BOUNDARY, WA          ',\n",
    "   'CUR'    :    'CURLEW, WA (BPS)',\n",
    "   'DVL'    :    'DANVILLE, WA          ',\n",
    "   'EVE'    :    'EVERETT, WA           ',\n",
    "   'FER'    :    'FERRY, WA             ',\n",
    "   'FRI'    :    'FRIDAY HARBOR, WA     ',\n",
    "   'FWA'    :    'FRONTIER, WA          ',\n",
    "   'KLM'    :    'KALAMA, WA            ',\n",
    "   'LAU'    :    'LAURIER, WA           ',\n",
    "   'LON'    :    'LONGVIEW, WA          ',\n",
    "   'MET'    :    'METALINE FALLS, WA    ',\n",
    "   'MWH'    :    'MOSES LAKE GRANT COUNTY ARPT, WA',\n",
    "   'NEA'    :    'NEAH BAY, WA          ',\n",
    "   'NIG'    :    'NIGHTHAWK, WA         ',\n",
    "   'OLY'    :    'OLYMPIA, WA           ',\n",
    "   'ORO'    :    'OROVILLE, WA          ',\n",
    "   'PWB'    :    'PASCO, WA             ',\n",
    "   'PIR'    :    'POINT ROBERTS, WA     ',\n",
    "   'PNG'    :    'PORT ANGELES, WA      ',\n",
    "   'PTO'    :    'PORT TOWNSEND, WA     ',\n",
    "   'SEA'    :    'SEATTLE, WA           ',\n",
    "   'SPO'    :    'SPOKANE, WA           ',\n",
    "   'SUM'    :    'SUMAS, WA             ',\n",
    "   'TAC'    :    'TACOMA, WA            ',\n",
    "   'PSC'    :    'TRI-CITIES - PASCO, WA',\n",
    "   'VAN'    :    'VANCOUVER, WA         ',\n",
    "   'AGM'    :    'ALGOMA, WI            ',\n",
    "   'BAY'    :    'BAYFIELD, WI          ',\n",
    "   'GRB'    :    'GREEN BAY, WI         ',\n",
    "   'MNW'    :    'MANITOWOC, WI         ',\n",
    "   'MIL'    :    'MILWAUKEE, WI         ',\n",
    "   'MSN'    :    'TRUAX FIELD - DANE COUNTY, WI',\n",
    "   'CHS'    :    'CHARLESTON, WV        ',\n",
    "   'CLK'    :    'CLARKSBURG, WV        ',\n",
    "   'BLF'    :    'MERCER COUNTY, WV',\n",
    "   'CSP'    :    'CASPER, WY            ',\n",
    "   'XXX'    :    'NOT REPORTED/UNKNOWN  ' ,\n",
    "   '888'    :    'UNIDENTIFED AIR / SEAPORT',\n",
    "   'UNK'    :    'UNKNOWN POE           ',\n",
    "   'CLG'    :    'CALGARY, CANADA       ',\n",
    "   'EDA'    :    'EDMONTON, CANADA      ',\n",
    "   'YHC'    :    'HAKAI PASS, CANADA',\n",
    "   'HAL'    :    'Halifax, NS, Canada   ',\n",
    "   'MON'    :    'MONTREAL, CANADA      ',\n",
    "   'OTT'    :    'OTTAWA, CANADA        ',\n",
    "   'YXE'    :    'SASKATOON, CANADA',\n",
    "   'TOR'    :    'TORONTO, CANADA       ',\n",
    "   'VCV'    :    'VANCOUVER, CANADA     ',\n",
    "   'VIC'    :    'VICTORIA, CANADA      ',\n",
    "   'WIN'    :    'WINNIPEG, CANADA      ',\n",
    "   'AMS'    :    'AMSTERDAM-SCHIPHOL, NETHERLANDS',\n",
    "   'ARB'    :    'ARUBA, NETH ANTILLES  ',\n",
    "   'BAN'    :    'BANKOK, THAILAND      ',\n",
    "   'BEI'    :    'BEICA #ARPT, ETHIOPIA',\n",
    "   'PEK'    :    'BEIJING CAPITAL INTL, PRC',\n",
    "   'BDA'    :    'KINDLEY FIELD, BERMUDA',\n",
    "   'BOG'    :    'BOGOTA, EL DORADO #ARPT, COLOMBIA',\n",
    "   'EZE'    :    'BUENOS AIRES, MINISTRO PIST, ARGENTINA',\n",
    "   'CUN'    :    'CANCUN, MEXICO',\n",
    "   'CRQ'    :    'CARAVELAS, BA #ARPT, BRAZIL',\n",
    "   'MVD'    :    'CARRASCO, URUGUAY',\n",
    "   'DUB'    :    'DUBLIN, IRELAND       ',\n",
    "   'FOU'    :    'FOUGAMOU #ARPT, GABON',\n",
    "   'FBA'    :    'FREEPORT, BAHAMAS      ',\n",
    "   'MTY'    :    'GEN M. ESCOBEDO, Monterrey, MX',\n",
    "   'HMO'    :    'GEN PESQUEIRA GARCIA, MX',\n",
    "   'GCM'    :    'GRAND CAYMAN, CAYMAN ISLAND',\n",
    "   'GDL'    :    'GUADALAJARA, MIGUEL HIDAL, MX',\n",
    "   'HAM'    :    'HAMILTON, BERMUDA     ',\n",
    "   'ICN'    :    'INCHON, SEOUL KOREA',\n",
    "   'IWA'    :    'INVALID - IWAKUNI, JAPAN',\n",
    "   'CND'    :    'KOGALNICEANU, ROMANIA',\n",
    "   'LAH'    :    'LABUHA ARPT, INDONESIA',\n",
    "   'DUR'    :    'LOUIS BOTHA, SOUTH AFRICA',\n",
    "   'MAL'    :    'MANGOLE ARPT, INDONESIA',\n",
    "   'MDE'    :    'MEDELLIN, COLOMBIA',\n",
    "   'MEX'    :    'JUAREZ INTL, MEXICO CITY, MX',\n",
    "   'LHR'    :    'MIDDLESEX, ENGLAND',\n",
    "   'NBO'    :    'NAIROBI, KENYA        ',\n",
    "   'NAS'    :    'NASSAU, BAHAMAS       ',\n",
    "   'NCA'    :    'NORTH CAICOS, TURK & CAIMAN',\n",
    "   'PTY'    :    'OMAR TORRIJOS, PANAMA',\n",
    "   'SPV'    :    'PAPUA, NEW GUINEA',\n",
    "   'UIO'    :    'QUITO (MARISCAL SUCR), ECUADOR',\n",
    "   'RIT'    :    'ROME, ITALY           ',\n",
    "   'SNO'    :    'SAKON NAKHON #ARPT, THAILAND',\n",
    "   'SLP'    :    'SAN LUIS POTOSI #ARPT, MEXICO',\n",
    "   'SAN'    :    'SAN SALVADOR, EL SALVADOR',\n",
    "   'SRO'    :    'SANTANA RAMOS #ARPT, COLOMBIA',\n",
    "   'GRU'    :    'GUARULHOS INTL, SAO PAULO, BRAZIL',\n",
    "   'SHA'    :    'SHANNON, IRELAND      ',\n",
    "   'HIL'    :    'SHILLAVO, ETHIOPIA',\n",
    "   'TOK'    :    'TOROKINA #ARPT, PAPUA, NEW GUINEA',\n",
    "   'VER'    :    'VERACRUZ, MEXICO',\n",
    "   'LGW'    :    'WEST SUSSEX, ENGLAND  ',\n",
    "   'ZZZ'    :    'MEXICO Land (Banco de Mexico) ',\n",
    "   'CHN'    :    'No PORT Code (CHN)',\n",
    "   'CNC'    :    'CANNON CORNERS, NY',\n",
    "   'MAA'    :    'Abu Dhabi',\n",
    "   'AG0'    :    'MAGNOLIA, AR',\n",
    "   'BHM'    :    'BAR HARBOR, ME',\n",
    "   'BHX'    :    'BIRMINGHAM, AL',\n",
    "   'CAK'    :    'AKRON, OH',\n",
    "   'FOK'    :    'SUFFOLK COUNTY, NY',\n",
    "   'LND'    :    'LANDER, WY',\n",
    "   'MAR'    :    'MARFA, TX',\n",
    "   'MLI'    :    'MOLINE, IL',\n",
    "   'RIV'    :    'RIVERSIDE, CA',\n",
    "   'RME'    :    'ROME, NY',\n",
    "   'VNY'    :    'VAN NUYS, CA',\n",
    "   'YUM'    :    'YUMA, AZ',\n",
    "   'FRG'    :    'Collapsed (FOK) 06/15',\n",
    "   'HRL'    :    'Collapsed (HLG) 06/15',\n",
    "   'ISP'    :    'Collapsed (FOK) 06/15',\n",
    "   'JSJ'    :    'Collapsed (SAJ) 06/15',\n",
    "   'BUS'    :    'Collapsed (BUF) 06/15',\n",
    "   'IAG'    :    'Collapsed (NIA) 06/15',\n",
    "   'PHN'    :    'Collapsed (PHU) 06/15',\n",
    "   'STN'    :    'Collapsed (STR) 06/15',\n",
    "   'VMB'    :    'Collapsed (VNB) 06/15',\n",
    "   'T01'    :    'Collapsed (SEA) 06/15',\n",
    "   'PHF'    :    'No PORT Code (PHF)',\n",
    "   'DRV'    :    'No PORT Code (DRV)',\n",
    "   'FTB'    :    'No PORT Code (FTB)',\n",
    "   'GAC'    :    'No PORT Code (GAC)',\n",
    "   'GMT'    :    'No PORT Code (GMT)',\n",
    "   'JFA'    :    'No PORT Code (JFA)',\n",
    "   'JMZ'    :    'No PORT Code (JMZ)',\n",
    "   'NC8'    :    'No PORT Code (NC8)',\n",
    "   'NYL'    :    'No PORT Code (NYL)',\n",
    "   'OAI'    :    'No PORT Code (OAI)',\n",
    "   'PCW'    :    'No PORT Code (PCW)',\n",
    "   'WA5'    :    'No PORT Code (WAS)',\n",
    "   'WTR'    :    'No PORT Code (WTR)',\n",
    "   'X96'    :    'No PORT Code (X96)',\n",
    "   'XNA'    :    'No PORT Code (XNA)',\n",
    "   'YGF'    :    'No PORT Code (YGF)',\n",
    "   '5T6'    :    'No PORT Code (5T6)',\n",
    "   '060'    :    'No PORT Code (60)',\n",
    "   'SP0'    :    'No PORT Code (SP0)',\n",
    "   'W55'    :    'No PORT Code (W55)',\n",
    "   'X44'    :    'No PORT Code (X44)',\n",
    "   'AUH'    :    'No PORT Code (AUH)',\n",
    "   'RYY'    :    'No PORT Code (RYY)',\n",
    "   'SUS'    :    'No PORT Code (SUS)',\n",
    "   '74S'    :    'No PORT Code (74S)',\n",
    "   'ATW'    :    'No PORT Code (ATW)',\n",
    "   'CPX'    :    'No PORT Code (CPX)',\n",
    "   'MTH'    :    'No PORT Code (MTH)',\n",
    "   'PFN'    :    'No PORT Code (PFN)',\n",
    "   'SCH'    :    'No PORT Code (SCH)',\n",
    "   'ASI'    :    'No PORT Code (ASI)',\n",
    "   'BKF'    :    'No PORT Code (BKF)',\n",
    "   'DAY'    :    'No PORT Code (DAY)',\n",
    "   'Y62'    :    'No PORT Code (Y62)',\n",
    "   'AG'        :    'No PORT Code (AG)',\n",
    "   'BCM'    :    'No PORT Code (BCM)',\n",
    "   'DEC'    :    'No PORT Code (DEC)',\n",
    "   'PLB'    :    'No PORT Code (PLB)',\n",
    "   'CXO'    :    'No PORT Code (CXO)',\n",
    "   'JBQ'    :    'No PORT Code (JBQ)',\n",
    "   'JIG'    :    'No PORT Code (JIG)',\n",
    "   'OGS'    :    'No PORT Code (OGS)',\n",
    "   'TIW'    :    'No PORT Code (TIW)',\n",
    "   'OTS'    :    'No PORT Code (OTS)',\n",
    "   'AMT'    :    'No PORT Code (AMT)',\n",
    "   'EGE'    :    'No PORT Code (EGE)',\n",
    "   'GPI'    :    'No PORT Code (GPI)',\n",
    "   'NGL'    :    'No PORT Code (NGL)',\n",
    "   'OLM'    :    'No PORT Code (OLM)',\n",
    "   '.GA'    :    'No PORT Code (.GA)',\n",
    "   'CLX'    :    'No PORT Code (CLX)',\n",
    "   'CP '    :    'No PORT Code (CP)',\n",
    "   'FSC'    :    'No PORT Code (FSC)',\n",
    "   'NK'     :    'No PORT Code (NK)',\n",
    "   'ADU'    :    'No PORT Code (ADU)',\n",
    "   'AKT'    :    'No PORT Code (AKT)',\n",
    "   'LIT'    :    'No PORT Code (LIT)',\n",
    "   'A2A'    :    'No PORT Code (A2A)',\n",
    "   'OSN'    :    'No PORT Code (OSN)'\n",
    "}\n",
    "df_ports = spark.createDataFrame(list(map(list, ports_codes.items())),\n",
    "                                         [\"port_code\", \"port_name\"])\n",
    "df_ports.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Read - Airports codes csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_df = spark.read.options(delimiter=\",\").csv(\"airport-codes_csv.csv\",header=True)\n",
    "a_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|TotalRecords|\n",
      "+------------+\n",
      "|       55075|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_df.createOrReplaceTempView(\"AirportTable\")\n",
    "aSQL = spark.sql(\"select count(*) AS TotalRecords from AirportTable\")\n",
    "aSQL.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|NotNullIataCodes|\n",
      "+----------------+\n",
      "|            9189|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aSQL = spark.sql(\"select count(*) as NotNullIataCodes from AirportTable WHERE iata_code IS NOT NULL\")\n",
    "aSQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Get count of records by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|          type|TypeCount|\n",
      "+--------------+---------+\n",
      "| large_airport|      602|\n",
      "| seaplane_base|      143|\n",
      "|      heliport|       68|\n",
      "|        closed|      279|\n",
      "|medium_airport|     3859|\n",
      "| small_airport|     4238|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aSQL = spark.sql(\"select type, count(*) as TypeCount from AirportTable WHERE iata_code IS NOT NULL GROUP BY type \")\n",
    "aSQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------+\n",
      "|iata_code|          type|count(1)|\n",
      "+---------+--------------+--------+\n",
      "|      LMC| small_airport|       2|\n",
      "|      RZS| small_airport|       2|\n",
      "|      MXR| small_airport|       2|\n",
      "|      YTY|medium_airport|       2|\n",
      "|      ZRH| large_airport|       2|\n",
      "|      RMD| small_airport|       2|\n",
      "|      REQ| small_airport|       2|\n",
      "|      HLA|medium_airport|       2|\n",
      "|      NWT| small_airport|       2|\n",
      "|      KCZ|medium_airport|       2|\n",
      "|      PCO| small_airport|       2|\n",
      "|      DZI| small_airport|       2|\n",
      "|      DDU| small_airport|       2|\n",
      "|      LPE| small_airport|       2|\n",
      "|      KMM| small_airport|       2|\n",
      "|      ULG| small_airport|       2|\n",
      "|      GVA| large_airport|       2|\n",
      "|      JNB| large_airport|       2|\n",
      "|      IST| large_airport|       2|\n",
      "|      IZA|medium_airport|       2|\n",
      "+---------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aSQL = spark.sql(\"select iata_code,type,count(*) from AirportTable \\\n",
    "                 WHERE type <>  'closed' and iata_code <> '0' \\\n",
    "                 group by iata_code,type having count(*) > 1 \")\n",
    "aSQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The airports csv file has a total of 55075 records. However, there are only 9189 records that have the iata code populated. This is the column that can be mapped to the immigration data. Also, there are 279 closed airports that can be filtered out. There are a number of airports that are duplicated as can be seen from the above pandas dataframe, these need to be removed as well. The latitude and longitude are given together a string column, these need to be separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clean up - Airports csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_column(value, split_character=\"-\",extract_index=0,output_type=\"str\"):\n",
    "    \"\"\"\n",
    "    Function is used to split the value (column supplied) and return a particular split (based on index supplied)\n",
    "    and outputs it to a string, float or int\n",
    "    \"\"\"\n",
    "    \n",
    "    splits = [eval(output_type)(val) for val in value.split(split_character)]\n",
    "    return splits[extract_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#user defined functions\n",
    "split_to_string = udf(split_column, StringType())\n",
    "split_to_float = udf(split_column,FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "a_df = a_df.withColumn(\"latitude\", split_to_float(a_df.coordinates,lit(\",\"),lit(0),lit(\"float\"))) \\\n",
    ".withColumn(\"longitude\",split_to_float(a_df.coordinates,lit(\",\"),lit(1),lit(\"float\"))) \\\n",
    ".withColumn(\"region\",split_to_string(a_df.iso_region,lit(\"-\"),lit(1),lit(\"str\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "a_df = a_df.dropDuplicates(subset=[\"iata_code\",\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_id</th>\n",
       "      <th>airport_type</th>\n",
       "      <th>airport_name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YNUM</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Numbulwar Airport</td>\n",
       "      <td>31</td>\n",
       "      <td>OC</td>\n",
       "      <td>AU</td>\n",
       "      <td>NT</td>\n",
       "      <td>None</td>\n",
       "      <td>YNUM</td>\n",
       "      <td>NUB</td>\n",
       "      <td>None</td>\n",
       "      <td>135.716995</td>\n",
       "      <td>-14.27170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZHLY</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Luoyang Airport</td>\n",
       "      <td>840</td>\n",
       "      <td>AS</td>\n",
       "      <td>CN</td>\n",
       "      <td>41</td>\n",
       "      <td>Luoyang</td>\n",
       "      <td>ZHLY</td>\n",
       "      <td>LYA</td>\n",
       "      <td>None</td>\n",
       "      <td>112.388000</td>\n",
       "      <td>34.74110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MH-LML</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lae Island Airport</td>\n",
       "      <td>None</td>\n",
       "      <td>OC</td>\n",
       "      <td>MH</td>\n",
       "      <td>LAE</td>\n",
       "      <td>Lae Island</td>\n",
       "      <td>None</td>\n",
       "      <td>LML</td>\n",
       "      <td>None</td>\n",
       "      <td>166.264999</td>\n",
       "      <td>8.92111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCTS</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>Tenerife South Airport</td>\n",
       "      <td>209</td>\n",
       "      <td>EU</td>\n",
       "      <td>ES</td>\n",
       "      <td>CN</td>\n",
       "      <td>Tenerife Island</td>\n",
       "      <td>GCTS</td>\n",
       "      <td>TFS</td>\n",
       "      <td>None</td>\n",
       "      <td>-16.572500</td>\n",
       "      <td>28.04450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYKS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Karasabai Airport</td>\n",
       "      <td>731</td>\n",
       "      <td>SA</td>\n",
       "      <td>GY</td>\n",
       "      <td>UT</td>\n",
       "      <td>Karasabai</td>\n",
       "      <td>SYKS</td>\n",
       "      <td>KRG</td>\n",
       "      <td>None</td>\n",
       "      <td>-59.533298</td>\n",
       "      <td>4.03333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airport_id   airport_type            airport_name elevation_ft continent  \\\n",
       "0       YNUM  small_airport       Numbulwar Airport           31        OC   \n",
       "1       ZHLY  small_airport         Luoyang Airport          840        AS   \n",
       "2     MH-LML  small_airport      Lae Island Airport         None        OC   \n",
       "3       GCTS  large_airport  Tenerife South Airport          209        EU   \n",
       "4       SYKS  small_airport       Karasabai Airport          731        SA   \n",
       "\n",
       "  iso_country region     municipality gps_code iata_code local_code  \\\n",
       "0          AU     NT             None     YNUM       NUB       None   \n",
       "1          CN     41          Luoyang     ZHLY       LYA       None   \n",
       "2          MH    LAE       Lae Island     None       LML       None   \n",
       "3          ES     CN  Tenerife Island     GCTS       TFS       None   \n",
       "4          GY     UT        Karasabai     SYKS       KRG       None   \n",
       "\n",
       "     latitude  longitude  \n",
       "0  135.716995  -14.27170  \n",
       "1  112.388000   34.74110  \n",
       "2  166.264999    8.92111  \n",
       "3  -16.572500   28.04450  \n",
       "4  -59.533298    4.03333  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_df.createOrReplaceTempView(\"AirportTable\")\n",
    "aSQL = spark.sql(\"select distinct  \\\n",
    "                         ident AS airport_id, \\\n",
    "                         type AS airport_type, \\\n",
    "                         name AS airport_name, \\\n",
    "                         elevation_ft, \\\n",
    "                         continent, \\\n",
    "                         iso_country, \\\n",
    "                         region, \\\n",
    "                         municipality, \\\n",
    "                         gps_code, \\\n",
    "                         iata_code, \\\n",
    "                         local_code, \\\n",
    "                         latitude, \\\n",
    "                         longitude \\\n",
    "                 from AirportTable \\\n",
    "                 WHERE iata_code is not null and iata_code <> '0'  \\\n",
    "                 and type <> 'closed'  \\\n",
    "                \")\n",
    "aSQL.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_g = aSQL.groupby('iata_code','airport_type').count()\n",
    "df_g = df_g.select(\"count\").filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read - Global Temperatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temp_df = spark.read.options(delimiter=\",\").csv(fname,header=True)\n",
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 8599212|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.createOrReplaceTempView(\"CityTempTable\")\n",
    "tSQL = spark.sql(''' SELECT count(*) FROM CityTempTable\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>min(DateRec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chad</td>\n",
       "      <td>1856-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russia</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paraguay</td>\n",
       "      <td>1832-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>1864-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senegal</td>\n",
       "      <td>1849-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Guyana</td>\n",
       "      <td>1824-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Burma</td>\n",
       "      <td>1796-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>1825-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eritrea</td>\n",
       "      <td>1864-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Djibouti</td>\n",
       "      <td>1864-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>1825-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>1825-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Malawi</td>\n",
       "      <td>1850-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Iraq</td>\n",
       "      <td>1808-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Germany</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1833-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cambodia</td>\n",
       "      <td>1825-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jordan</td>\n",
       "      <td>1808-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rwanda</td>\n",
       "      <td>1850-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sudan</td>\n",
       "      <td>1850-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>France</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Greece</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>1796-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Taiwan</td>\n",
       "      <td>1841-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>1753-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>1856-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Togo</td>\n",
       "      <td>1849-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Slovakia</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Haiti</td>\n",
       "      <td>1823-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Poland</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Australia</td>\n",
       "      <td>1841-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Cameroon</td>\n",
       "      <td>1856-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Papua New Guinea</td>\n",
       "      <td>1882-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Romania</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Nepal</td>\n",
       "      <td>1796-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Austria</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>1779-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>1791-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>El Salvador</td>\n",
       "      <td>1836-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>1851-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Serbia</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>1857-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Burkina Faso</td>\n",
       "      <td>1849-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Bahrain</td>\n",
       "      <td>1843-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Colombia</td>\n",
       "      <td>1824-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>1796-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Guinea Bissau</td>\n",
       "      <td>1849-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Bosnia And Herzegovina</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Mauritius</td>\n",
       "      <td>1787-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Moldova</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Congo (Democratic Republic Of The)</td>\n",
       "      <td>1850-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>1825-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Mali</td>\n",
       "      <td>1849-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Country min(DateRec)\n",
       "0                                  Chad   1856-01-01\n",
       "1                                Russia   1743-11-01\n",
       "2                              Paraguay   1832-01-01\n",
       "3                                 Yemen   1864-01-01\n",
       "4                               Senegal   1849-01-01\n",
       "5                                Sweden   1743-11-01\n",
       "6                                Guyana   1824-01-01\n",
       "7                                 Burma   1796-01-01\n",
       "8                           Philippines   1825-01-01\n",
       "9                               Eritrea   1864-01-01\n",
       "10                             Djibouti   1864-01-01\n",
       "11                             Malaysia   1825-01-01\n",
       "12                            Singapore   1825-01-01\n",
       "13                               Turkey   1743-11-01\n",
       "14                               Malawi   1850-01-01\n",
       "15                                 Iraq   1808-10-01\n",
       "16                              Germany   1743-11-01\n",
       "17                          Afghanistan   1833-01-01\n",
       "18                             Cambodia   1825-01-01\n",
       "19                               Jordan   1808-10-01\n",
       "20                               Rwanda   1850-01-01\n",
       "21                                Sudan   1850-01-01\n",
       "22                               France   1743-11-01\n",
       "23                               Greece   1743-11-01\n",
       "24                            Sri Lanka   1796-01-01\n",
       "25                               Taiwan   1841-01-01\n",
       "26                              Algeria   1753-01-01\n",
       "27                    Equatorial Guinea   1856-01-01\n",
       "28                                 Togo   1849-01-01\n",
       "29                             Slovakia   1743-11-01\n",
       "..                                  ...          ...\n",
       "129                               Haiti   1823-01-01\n",
       "130                              Poland   1743-11-01\n",
       "131                            Portugal   1743-11-01\n",
       "132                           Australia   1841-01-01\n",
       "133                            Cameroon   1856-01-01\n",
       "134                    Papua New Guinea   1882-01-01\n",
       "135                             Romania   1743-11-01\n",
       "136                            Bulgaria   1743-11-01\n",
       "137                               Nepal   1796-01-01\n",
       "138                             Austria   1743-11-01\n",
       "139                          Kazakhstan   1779-11-01\n",
       "140                               Egypt   1791-05-01\n",
       "141                         El Salvador   1836-01-01\n",
       "142                          Costa Rica   1851-01-01\n",
       "143                              Serbia   1743-11-01\n",
       "144                        South Africa   1857-01-01\n",
       "145                        Burkina Faso   1849-01-01\n",
       "146                             Bahrain   1843-01-01\n",
       "147                            Colombia   1824-01-01\n",
       "148                             Hungary   1743-11-01\n",
       "149                            Pakistan   1796-01-01\n",
       "150                       Guinea Bissau   1849-01-01\n",
       "151              Bosnia And Herzegovina   1743-11-01\n",
       "152                           Mauritius   1787-01-01\n",
       "153                      United Kingdom   1743-11-01\n",
       "154                             Moldova   1743-11-01\n",
       "155  Congo (Democratic Republic Of The)   1850-01-01\n",
       "156                             Vietnam   1825-01-01\n",
       "157                         Netherlands   1743-11-01\n",
       "158                                Mali   1849-01-01\n",
       "\n",
       "[159 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tSQL = spark.sql('''\n",
    "                     select date(dt) DateRec ,\n",
    "                            date_format(date(dt),'MMM') Month,\n",
    "                            float(AverageTemperature) AvgTemp,\n",
    "                            City, \n",
    "                            Country\n",
    "                     from CityTempTable\n",
    "                   \n",
    "                     \n",
    "''')\n",
    "#tSQL.printSchema()\n",
    "result = tSQL.groupby(\"Country\").agg({'DateRec': 'min'})\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min(DateRec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  min(DateRec)\n",
       "0   1743-11-01"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get minimum date recorded for cities in united States. \n",
    "tSQL = spark.sql('''\n",
    "                     select date(dt) DateRec ,\n",
    "                            date_format(date(dt),'MMM') Month,\n",
    "                            float(AverageTemperature) AvgTemp,\n",
    "                            City, \n",
    "                            Country\n",
    "                     from CityTempTable\n",
    "                     WHERE Country = 'United States'          \n",
    "''')\n",
    "#tSQL.printSchema()\n",
    "result = tSQL.agg({'DateRec': 'min'})\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>AvgTempJan</th>\n",
       "      <th>AvgTempFeb</th>\n",
       "      <th>AvgTempMar</th>\n",
       "      <th>AvgTempApr</th>\n",
       "      <th>AvgTempMay</th>\n",
       "      <th>AvgTempJun</th>\n",
       "      <th>AvgTempJul</th>\n",
       "      <th>AvgTempAug</th>\n",
       "      <th>AvgTempSep</th>\n",
       "      <th>AvgTempOct</th>\n",
       "      <th>AvgTempNov</th>\n",
       "      <th>AvgTempDec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Allentown</td>\n",
       "      <td>United States</td>\n",
       "      <td>-2.259500</td>\n",
       "      <td>-0.886429</td>\n",
       "      <td>4.095786</td>\n",
       "      <td>10.117500</td>\n",
       "      <td>15.698071</td>\n",
       "      <td>20.634857</td>\n",
       "      <td>23.144786</td>\n",
       "      <td>22.391571</td>\n",
       "      <td>18.340000</td>\n",
       "      <td>11.544000</td>\n",
       "      <td>6.458846</td>\n",
       "      <td>0.357846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pueblo</td>\n",
       "      <td>United States</td>\n",
       "      <td>-0.245714</td>\n",
       "      <td>1.001786</td>\n",
       "      <td>6.420214</td>\n",
       "      <td>10.885214</td>\n",
       "      <td>16.350857</td>\n",
       "      <td>22.149000</td>\n",
       "      <td>24.956714</td>\n",
       "      <td>23.557000</td>\n",
       "      <td>18.876214</td>\n",
       "      <td>11.512154</td>\n",
       "      <td>5.105077</td>\n",
       "      <td>-0.627692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.986714</td>\n",
       "      <td>2.786143</td>\n",
       "      <td>4.269214</td>\n",
       "      <td>6.445857</td>\n",
       "      <td>9.989214</td>\n",
       "      <td>12.945143</td>\n",
       "      <td>16.251572</td>\n",
       "      <td>16.121785</td>\n",
       "      <td>13.448571</td>\n",
       "      <td>8.252462</td>\n",
       "      <td>4.083769</td>\n",
       "      <td>1.492308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Garden Grove</td>\n",
       "      <td>United States</td>\n",
       "      <td>14.233929</td>\n",
       "      <td>13.531000</td>\n",
       "      <td>14.312571</td>\n",
       "      <td>14.741429</td>\n",
       "      <td>16.715214</td>\n",
       "      <td>17.955929</td>\n",
       "      <td>19.937214</td>\n",
       "      <td>20.690857</td>\n",
       "      <td>20.813500</td>\n",
       "      <td>18.533461</td>\n",
       "      <td>16.491154</td>\n",
       "      <td>13.814308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Huntington Beach</td>\n",
       "      <td>United States</td>\n",
       "      <td>14.233929</td>\n",
       "      <td>13.531000</td>\n",
       "      <td>14.312571</td>\n",
       "      <td>14.741429</td>\n",
       "      <td>16.715214</td>\n",
       "      <td>17.955929</td>\n",
       "      <td>19.937214</td>\n",
       "      <td>20.690857</td>\n",
       "      <td>20.813500</td>\n",
       "      <td>18.533461</td>\n",
       "      <td>16.491154</td>\n",
       "      <td>13.814308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City        Country  AvgTempJan  AvgTempFeb  AvgTempMar  \\\n",
       "0         Allentown  United States   -2.259500   -0.886429    4.095786   \n",
       "1            Pueblo  United States   -0.245714    1.001786    6.420214   \n",
       "2           Seattle  United States    1.986714    2.786143    4.269214   \n",
       "3      Garden Grove  United States   14.233929   13.531000   14.312571   \n",
       "4  Huntington Beach  United States   14.233929   13.531000   14.312571   \n",
       "\n",
       "   AvgTempApr  AvgTempMay  AvgTempJun  AvgTempJul  AvgTempAug  AvgTempSep  \\\n",
       "0   10.117500   15.698071   20.634857   23.144786   22.391571   18.340000   \n",
       "1   10.885214   16.350857   22.149000   24.956714   23.557000   18.876214   \n",
       "2    6.445857    9.989214   12.945143   16.251572   16.121785   13.448571   \n",
       "3   14.741429   16.715214   17.955929   19.937214   20.690857   20.813500   \n",
       "4   14.741429   16.715214   17.955929   19.937214   20.690857   20.813500   \n",
       "\n",
       "   AvgTempOct  AvgTempNov  AvgTempDec  \n",
       "0   11.544000    6.458846    0.357846  \n",
       "1   11.512154    5.105077   -0.627692  \n",
       "2    8.252462    4.083769    1.492308  \n",
       "3   18.533461   16.491154   13.814308  \n",
       "4   18.533461   16.491154   13.814308  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter by only dates greater than or equal to year 2000 and for cities in United States Only\n",
    "# as we are not interested in temperatures of other cities\n",
    "# City can then be joined to the demographics table !\n",
    "temp_df.createOrReplaceTempView(\"CityTempTable\")\n",
    "tSQL = spark.sql('''\n",
    "                     select date(dt) DateRec ,\n",
    "                            date_format(date(dt),'MMM') Month,\n",
    "                            float(AverageTemperature) AvgTemp,\n",
    "                            City, \n",
    "                            Country\n",
    "                     from CityTempTable where to_date(dt,'yyyy-MM-dd') >= '2000-01-01' \n",
    "                     and Country = 'United States' \n",
    "                  ''')\n",
    "\n",
    "#now get the average temperature per month for each City across the years!\n",
    "# Pivot the dataframe by Month to get average temp in a single row for each row \n",
    "t_pivot_df = tSQL.groupBy(\"City\",\"Country\").pivot(\"Month\").avg(\"AvgTemp\")\n",
    "\n",
    "#join the two datasets to create a single combined final dataset and drop duplicates\n",
    "f_df = tSQL.join(t_pivot_df,[\"City\",\"Country\"]).drop(\"DateRec\",\"AvgTemp\",\"Month\").dropDuplicates()\n",
    "\n",
    "# Create final view  with renamed columns which will be written to parquet\n",
    "f_df.createOrReplaceTempView(\"FinalCityTempTable\")\n",
    "fSQL = spark.sql('''\n",
    "                  select City,  \n",
    "                         Country, \n",
    "                         Jan AvgTempJan,\n",
    "                         Feb AvgTempFeb,\n",
    "                         Mar AvgTempMar,\n",
    "                         Apr AvgTempApr,\n",
    "                         May AvgTempMay,\n",
    "                         Jun AvgTempJun,\n",
    "                         Jul AvgTempJul,\n",
    "                         Aug AvgTempAug,\n",
    "                         Sep AvgTempSep,\n",
    "                         Oct AvgTempOct,\n",
    "                         Nov AvgTempNov,\n",
    "                         Dec AvgTempDec\n",
    "                 from FinalCityTempTable\n",
    "''')\n",
    "fSQL.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Read Immigration data and build dates table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month_name</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_week_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>20160301</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>March</td>\n",
       "      <td>3</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>20160425</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>April</td>\n",
       "      <td>2</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-05-03</td>\n",
       "      <td>20160503</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>May</td>\n",
       "      <td>3</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-15</td>\n",
       "      <td>20160815</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>August</td>\n",
       "      <td>2</td>\n",
       "      <td>Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>20160831</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>August</td>\n",
       "      <td>4</td>\n",
       "      <td>Wed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   date_id  year  month  day  week month_name  day_of_week  \\\n",
       "0  2016-03-01  20160301  2016      3    1     9      March            3   \n",
       "1  2016-04-25  20160425  2016      4   25    17      April            2   \n",
       "2  2016-05-03  20160503  2016      5    3    18        May            3   \n",
       "3  2016-08-15  20160815  2016      8   15    33     August            2   \n",
       "4  2016-08-31  20160831  2016      8   31    35     August            4   \n",
       "\n",
       "  day_of_week_name  \n",
       "0              Tue  \n",
       "1              Mon  \n",
       "2              Tue  \n",
       "3              Mon  \n",
       "4              Wed  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read immigration data\n",
    "df = spark.read.parquet(\"sas_data\")\n",
    "\n",
    "# convert date fields to date type\n",
    "get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(float(x))).isoformat() if x else None)\n",
    "\n",
    "#convert sas dates to date\n",
    "df = df.withColumn(\"arrival_date\", get_date(df.arrdate))\n",
    "df = df.withColumn(\"departure_date\", get_date(df.depdate))\n",
    "\n",
    "df = df.withColumn(\"arrival_date\", to_date(df.arrival_date, 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"departure_date\", to_date(df.departure_date, 'yyyy-MM-dd'))\n",
    "\n",
    "#create dates dataframe with unique dates from the file\n",
    "df.createOrReplaceTempView(\"DatesData\")\n",
    "dtSQL = spark.sql('''\n",
    "                    select distinct date\n",
    "                    from \n",
    "                    (\n",
    "                        select arrival_date date from DatesData WHERE arrival_date >= to_date('2016-01-01')\n",
    "                        UNION\n",
    "                        select departure_date date from DatesData WHERE departure_date is not null and departure_date >= to_date('2016-01-01')\n",
    "                    ) as Drv         \n",
    "                ''')\n",
    "\n",
    "#add columns to enrich the dates dataframe. this is will be used to write to parquet\n",
    "dtSQL = dtSQL.select(\"date\", \\\n",
    "                     date_format(\"date\", 'yyyyMMdd').alias(\"date_id\"), \\\n",
    "                     year(\"date\").alias(\"year\"), \\\n",
    "                     month(\"date\").alias(\"month\"), \\\n",
    "                     dayofmonth(\"date\").alias(\"day\"), \\\n",
    "                     weekofyear(\"date\").alias(\"week\"), \\\n",
    "                     date_format(\"date\",'MMMM').alias(\"month_name\"), \\\n",
    "                     dayofweek(\"date\").alias(\"day_of_week\"),\\\n",
    "                     date_format(\"date\",'E').alias(\"day_of_week_name\")\n",
    "                    )\n",
    "dtSQL.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "I used a star schema for the data model. My intention is to keep the queries as simple as possible by not making the tables too normalized and limiting the required number of joins. An ER diagram of this data model can be found [here] ()  \n",
    "\n",
    "At the center of the data model is a fact table focusing on immigration (fact_immigration), the majority of which can be found in the immigration data. The dimensions provide additonal information to support the immigration data (dim_date, dim_country, dim_state, dim_city, dim_visa, dim_travel_mode, dim_ports). \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. etl.py - This python script was run to transform the various data files (immigration, airports, ports, demographics) into various parquet files that are stored in S3 bucket.  \n",
    "2. The data model is on AWS Redshift. I chose Redshift due to its efficiency in handling large amounts of data. \n",
    "The files in S3 are first staged and then loaded into the various dimensions and facts.  \n",
    "3. I chose Apache Airflow to complete this data pipeline because of its ease of use, has ready-to-use operators that can be used to integrate Airflow with S3 and Redshift. It's graphical UI can be used to monitor and manage work flows. It is easy to check the status of tasks and to schedule the tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.  \n",
    "\n",
    "Prerequisite: Redshift cluster created in Oregon region as S3 files are stored in Oregon region. Modify the accessibility settings of the cluster so that it is accessible from the internet. \n",
    "\n",
    "Once this is done, the following steps are executed.\n",
    "\n",
    "1. To create the required staging, dimension and fact tables in Redshift, execute the capstone_tablecreation_dag in the Airflow Web UI. This drops and creates all the tables required by the data model\n",
    "\n",
    "2. Next, execute the capstone_dag in the Airflow UI. This takes care of populating the staging tables from S3, transforms the data and loads to dimension tables and fact table. The data quality checks are also taken care of here.  \n",
    "\n",
    "The data pipeline in Airflow consists of the below operators:\n",
    "StageToRedshiftOperator  \n",
    "This operator takes a  parquet file and copies it directly into a Redshift table. All the staging tables below are loaded in parallel.\n",
    "\n",
    "staging_visa  \n",
    "staging_travel_mode  \n",
    "staging_country  \n",
    "staging_ports  \n",
    "staging_climate  \n",
    "staging_city  \n",
    "staging_state  \n",
    "staging_date  \n",
    "staging_immigration    \n",
    "\n",
    "LoadDimensionOperator\n",
    "This operator queries the staging tables and populates the dimension tables. It provides an optional parameter called columns that allows the user to specify the fields in which to enter data rather than assuming every field is being populated. There is a helper file that contains all the queries referenced in this operator. The following tables are populated using this.  \n",
    "\n",
    "dim_visa  \n",
    "dim_travel_mode  \n",
    "dim_country  \n",
    "dim_ports  \n",
    "dim_state  \n",
    "dim_city  \n",
    "dim_date  \n",
    "\n",
    "LoadFactOperator  \n",
    "This operator is used to populate the fact table fact_immigration. There is a helper file that contains all the queries referenced in this operator.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "DataQualityOperator  \n",
    "This takes in as a parameter a series of queries to run on the newly populated tables to ensure the data pipeline runs as expected. \n",
    "This operator accepts a parameter  is an expected value and any valid comparison (=, <>, >, <, >=, <=). This gives the user the flexibility when creating data quality checks. \n",
    "\n",
    "Checking that a table has at least a certain number of records  \n",
    "Ensuring that dimension keys are not null   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The data dictionary for the final data model can be found [here](documents/DataDictionary.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "Used spark to transform the data into parquet files that are stored in S3. Spark is a powerful analytical engine for big data and hence used this.   \n",
    "S3 facilitates highly-scalable, secured and low-latency data storage from the cloud. With its simple web service interface, it is easy to store and retrieve data on Amazon S3 from anywhere on the web.  \n",
    "Redshift is cloud based and hosted directly on Amazon Web Services and has a flexible architecture that can scale in seconds to meet changing storage needs. Costs can be kept relatively low and it is easy to use.   \n",
    "Airflow is used to automatically organise, execute, and monitor data flow. Hene, used airflow for executing the data pipeline  \n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "The I94 immigration data is updated on a monthly basis and hence it is feasible to say, data processing and ETL can be done on a monthly basis.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.    \n",
    " For the existing project, the Spark and Airflow processes are run in the Udacity workspace. If the data was increased by 100x, I would run these processes on a more powerful environment in AWS, such as Amazon Elastic MapReduce (EMR) for Spark and Amazon Managed Workflows for Apache Airflow (MWAA) for Airflow.  \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.  \n",
    " Schedule the dag to run every morning and set a SLA so that it completes in a reasonable amount of time.  \n",
    " * The database needed to be accessed by 100+ people.  \n",
    " Amazon Redshift data sharing lets us share live data in Amazon Redshift to securely and easily share data for read purposes with other Redshift clusters within and across AWS accounts and with AWS analytic services using the data lake. With data sharing, users can instantly query live data from any Redshift cluster as long as they have permissions to access without the complexity and delays associated with data copies and data movement. This feature can be used if the database has to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
